{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItWasAllYellow/NLP_2025/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145148bd",
      "metadata": {
        "id": "145148bd"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "In this assignment, you will explore about word vectors.\n",
        "\n",
        "- Submision: A report in ``pdf``, your completed notebook file in ``ipynb``, and training data in ``txt``\n",
        "    - The assignment will be evalulated mainly with report. So please include every detail you want to present in your report, including figures.\n",
        "    - Report: Free format. You can copy and paste part of your code for some problems.\n",
        "      - Report has to be written in English\n",
        "    - ipynb: Save your notebook (with output of each cell if possible) as ipynb and submit it\n",
        "- Evaluation criteria\n",
        "    - How interesting and original are the presented examples\n",
        "    - How well you describe the reason of success or failure of your examples by considering how Word2Vec is trained\n",
        "    - Any description that is suspicious for using LLM without understanding the content can be penalized. You may use LLM for translation, but you have to describe it in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ee4fd6",
      "metadata": {
        "id": "97ee4fd6"
      },
      "source": [
        "## 0. Setup\n",
        "- Check ``gensim`` library is installed\n",
        "  - if not, you can install using ``!pip install gensim``\n",
        "- List the downloadable vectors from ``gensim``\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "L2XCe8lZ68NX",
        "outputId": "41f442e2-c130-4446-a234-fcd79e14bfdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "L2XCe8lZ68NX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "db0bbbcb",
      "metadata": {
        "id": "db0bbbcb"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pprint as pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9bbeb288",
      "metadata": {
        "id": "9bbeb288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e1eb10-a39f-4c97-c7b6-50aa59b682bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fasttext-wiki-news-subwords-300',\n",
              " 'conceptnet-numberbatch-17-06-300',\n",
              " 'word2vec-ruscorpora-300',\n",
              " 'word2vec-google-news-300',\n",
              " 'glove-wiki-gigaword-50',\n",
              " 'glove-wiki-gigaword-100',\n",
              " 'glove-wiki-gigaword-200',\n",
              " 'glove-wiki-gigaword-300',\n",
              " 'glove-twitter-25',\n",
              " 'glove-twitter-50',\n",
              " 'glove-twitter-100',\n",
              " 'glove-twitter-200',\n",
              " '__testing_word2vec-matrix-synopsis']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "list(gensim.downloader.info()['models'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeae1e1d",
      "metadata": {
        "id": "aeae1e1d"
      },
      "source": [
        "- Among the Word2Vec model codes above, select one model of your choice among ``glove-wiki-gigaword`` or ``glove-twitter``\n",
        "    - numbers at the last represents the number of dimension of each Word2Vec Model\n",
        "        - e.g. ``glove-twitter-200`` was trained on twitter dataset while embedding each word into 200-dim vector\n",
        "        - e.g. ``glove-wiki-gigaword-300`` was trained on wikipedia dataset while embedding each word into 300-dim vector\n",
        "- Download the selected model and load it as a ``model``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a0ace8aa",
      "metadata": {
        "id": "a0ace8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16e1b00-91a1-4ab6-cb6f-16a1ba88e1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "your_model_code = 'glove-wiki-gigaword-300' # select among the model code aboves\n",
        "model = gensim.downloader.load(your_model_code) # download and load the model. It can take some time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "41125ee7",
      "metadata": {
        "id": "41125ee7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5548c894-a1b9-471d-f1d4-62d83d892780"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.29353  ,  0.33247  , -0.047372 , -0.12247  ,  0.071956 ,\n",
              "       -0.23408  , -0.06238  , -0.0037192, -0.39462  , -0.69411  ,\n",
              "        0.36731  , -0.12141  , -0.044485 , -0.15268  ,  0.34864  ,\n",
              "        0.22926  ,  0.54361  ,  0.25215  ,  0.097972 , -0.087305 ,\n",
              "        0.87058  , -0.12211  , -0.079825 ,  0.28712  , -0.68563  ,\n",
              "       -0.27265  ,  0.22056  , -0.75752  ,  0.56293  ,  0.091377 ,\n",
              "       -0.71004  , -0.3142   , -0.56826  , -0.26684  , -0.60102  ,\n",
              "        0.26959  , -0.17992  ,  0.10701  , -0.57858  ,  0.38161  ,\n",
              "       -0.67127  ,  0.10927  ,  0.079426 ,  0.022372 , -0.081147 ,\n",
              "        0.011182 ,  0.67089  , -0.19094  , -0.33676  , -0.48471  ,\n",
              "       -0.35406  , -0.15209  ,  0.44503  ,  0.46385  ,  0.38409  ,\n",
              "        0.045081 , -0.59079  ,  0.21763  ,  0.38576  , -0.44567  ,\n",
              "        0.009332 ,  0.442    ,  0.097062 ,  0.38005  , -0.11881  ,\n",
              "       -0.42718  , -0.31005  , -0.025058 ,  0.12689  , -0.13468  ,\n",
              "        0.11976  ,  0.76253  ,  0.2524   , -0.26934  ,  0.068629 ,\n",
              "       -0.10071  ,  0.011066 , -0.18532  ,  0.44983  , -0.57507  ,\n",
              "        0.12278  , -0.064878 ,  0.044456 , -0.020999 , -0.069838 ,\n",
              "       -0.47329  , -0.43074  ,  0.39158  , -0.047815 , -0.93659  ,\n",
              "       -0.55128  , -0.1422   , -0.15829  ,  0.15623  ,  0.070461 ,\n",
              "        0.19892  ,  0.18942  , -0.19339  , -0.46594  , -0.028825 ,\n",
              "        0.0056752, -0.0054038,  0.43144  ,  0.12257  , -0.2611   ,\n",
              "        0.04847  ,  0.32244  , -0.31064  , -0.10559  ,  0.97954  ,\n",
              "        0.069626 , -0.023187 , -0.86293  ,  0.48273  ,  0.23649  ,\n",
              "       -0.0034704, -0.18932  ,  0.18588  ,  0.023211 , -0.30643  ,\n",
              "       -0.35717  ,  0.19605  , -0.1584   , -0.0058626,  0.35248  ,\n",
              "        0.036053 , -0.53933  ,  0.49435  ,  0.45332  , -0.18477  ,\n",
              "        0.040648 , -0.094517 , -0.07116  ,  0.74005  , -0.11465  ,\n",
              "       -0.26916  ,  0.089765 , -0.25205  , -0.21469  , -0.38847  ,\n",
              "        0.32509  ,  0.25773  , -0.51764  , -0.38457  ,  0.028254 ,\n",
              "       -0.21232  , -0.27311  ,  0.69178  , -0.37681  ,  0.14241  ,\n",
              "       -0.24926  ,  0.40314  , -0.052916 ,  0.07684  ,  0.2135   ,\n",
              "        0.10921  ,  0.049658 ,  0.02093  ,  0.11953  ,  0.28648  ,\n",
              "        0.87791  ,  0.085838 ,  0.31983  ,  0.51856  , -0.22628  ,\n",
              "        0.12402  ,  0.48805  ,  0.22111  , -0.52021  ,  0.0025106,\n",
              "       -0.13305  , -0.052565 ,  0.32744  ,  0.64985  ,  0.072426 ,\n",
              "       -0.52743  , -0.20913  , -0.27897  , -0.10834  , -0.10103  ,\n",
              "        0.15299  , -0.36681  ,  0.082445 ,  0.1739   , -0.28099  ,\n",
              "       -0.069136 ,  0.7895   ,  0.060571 ,  0.38693  , -0.16495  ,\n",
              "       -0.21801  ,  0.33288  , -0.44568  , -0.49892  , -0.34438  ,\n",
              "       -0.035606 , -0.24239  , -0.4747   , -0.17254  ,  0.071349 ,\n",
              "        1.4091   ,  0.46166  ,  0.46546  , -0.30979  ,  0.37203  ,\n",
              "        0.47897  , -0.28872  , -0.65515  , -0.13629  , -0.14287  ,\n",
              "       -0.04843  , -0.12786  ,  0.18941  , -0.037051 ,  0.59471  ,\n",
              "       -0.0051618, -0.0086009, -0.33313  ,  0.288    , -0.058965 ,\n",
              "       -0.67275  ,  0.15544  ,  0.074187 , -0.36441  , -0.021285 ,\n",
              "       -0.065337 ,  0.13827  ,  0.008395 , -0.041113 ,  0.29401  ,\n",
              "       -0.10344  , -0.052371 , -0.63084  ,  0.16311  ,  0.052826 ,\n",
              "       -0.021797 , -0.28115  , -0.078361 , -0.38124  ,  0.078089 ,\n",
              "        0.38411  , -0.34629  , -0.4322   ,  0.091731 , -0.67867  ,\n",
              "       -0.041138 , -0.53981  ,  0.10678  ,  0.03343  ,  0.81396  ,\n",
              "       -0.19448  ,  0.026248 , -0.14215  ,  0.2954   ,  0.62738  ,\n",
              "        0.26499  ,  0.6191   , -0.04113  ,  0.12301  ,  0.3158   ,\n",
              "        0.10698  ,  0.023654 , -0.41355  ,  0.034852 ,  0.21361  ,\n",
              "        0.045834 ,  0.053415 , -0.36421  ,  0.19707  ,  0.50916  ,\n",
              "       -0.1949   , -0.18788  , -0.24449  , -0.63397  , -0.23125  ,\n",
              "       -0.18823  , -1.0601   ,  0.47794  , -1.0102   ,  0.24604  ,\n",
              "       -0.4876   ,  0.79146  , -0.11047  , -0.21762  , -0.6178   ,\n",
              "        0.27815  , -0.098169 , -0.063205 ,  0.066069 , -0.69305  ,\n",
              "       -0.25928  ,  0.44591  , -0.64198  , -0.33084  , -0.30154  ,\n",
              "       -0.56359  ,  0.60501  , -0.09673  ,  0.44444  ,  0.22007  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# test the model output\n",
        "model['cat']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c29f5df",
      "metadata": {
        "id": "1c29f5df"
      },
      "source": [
        "## Problem 1. Simple Mathematics with Word2Vec\n",
        "- In this problem, you have to complete the given functions ``word_analogy_with_vector`` and ``get_cosine_similarity``\n",
        "  - To get the exactly same result with ``model.most_similar()``, you have to normalize each vector before doing arithmetic.\n",
        "  - Using L2 norm (sqrt of sum of square of every item in the vector)\n",
        "  - The result will also naturally include the positive query words itsef.\n",
        "- In your report, **please include your code for these functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "472d0add",
      "metadata": {
        "id": "472d0add",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fe1e34-1b29-4603-93ef-7e1eff556565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result vector is  [-0.03121225 -0.04885079 -0.00052049  0.08669862  0.09005838 -0.06614241\n",
            "  0.03420759  0.00142278  0.01974262 -0.13622826 -0.07791571 -0.09958889\n",
            "  0.06360311  0.08859422  0.03489114  0.01636234  0.06570574  0.05126741\n",
            " -0.03242207 -0.04330669 -0.15668158  0.0368449  -0.02985314  0.01033623\n",
            "  0.03430319 -0.07671046 -0.04376435 -0.04739117  0.06318487  0.01455305\n",
            " -0.06891646 -0.05026019 -0.06613863  0.03281747 -0.14246     0.0012542\n",
            " -0.0561171   0.00502324  0.06503114  0.00781858  0.02678905 -0.0506545\n",
            " -0.01988168  0.05223122  0.02118688 -0.03840835  0.05407127  0.04449621\n",
            "  0.00837921 -0.10452366 -0.03278344  0.01043135  0.12034565 -0.10535439\n",
            " -0.06399182 -0.02953902  0.05116515  0.00461546  0.0933569   0.01697323\n",
            "  0.00224307 -0.04162461  0.01235028  0.07910962  0.04070729 -0.13898228\n",
            "  0.02305285 -0.00028938  0.01022689  0.08450352  0.004185   -0.073237\n",
            " -0.03146702  0.01343366  0.05988747  0.0379662   0.07760164 -0.10404901\n",
            "  0.03800888  0.00652905  0.05621087 -0.05703957  0.00052586  0.10210311\n",
            "  0.04439221 -0.02218275 -0.02383728  0.08062544  0.06664036 -0.03834471\n",
            " -0.02914053 -0.13057746 -0.06508699  0.0492184   0.00117717 -0.02216643\n",
            "  0.00351599 -0.00617467  0.00742526  0.01693079  0.06231264 -0.13107494\n",
            "  0.09238424 -0.09661935  0.01735212  0.00345032  0.06047598 -0.03172597\n",
            "  0.0428396   0.11235198  0.08565899  0.14536214  0.00353915  0.01609374\n",
            "  0.04224381 -0.01839703  0.06178828 -0.07071428 -0.00895877 -0.02799662\n",
            " -0.09373647  0.18997249 -0.07481438 -0.15087831 -0.04815839  0.05617552\n",
            "  0.0261135   0.00282335  0.05473723  0.05489159  0.06589463  0.07086328\n",
            " -0.02251394  0.0763407   0.00963536 -0.12860486 -0.0850881   0.0540632\n",
            " -0.04652632  0.09621212  0.06127914  0.11704555 -0.03273003 -0.02048638\n",
            " -0.00616311 -0.06192318  0.03464736  0.03356656  0.01360707 -0.01498676\n",
            "  0.05308044 -0.03268919 -0.0315714   0.08264498 -0.02090618  0.07997206\n",
            " -0.00544729 -0.05964592 -0.04174615 -0.03617422  0.1437106   0.09588765\n",
            "  0.02397013  0.03001864 -0.01288839  0.06782335 -0.0282574   0.02106398\n",
            "  0.03916037  0.1499702  -0.02869633  0.07621491 -0.04843862 -0.03194914\n",
            " -0.02486094  0.07784204 -0.08811422  0.00811917 -0.04478286  0.06412743\n",
            " -0.07215349  0.07983328 -0.04624705 -0.04270916  0.03409419 -0.05807603\n",
            " -0.07892573  0.07238391 -0.04373726 -0.15217087 -0.07448583 -0.02701609\n",
            " -0.0258029  -0.14638524 -0.00291746  0.1086662  -0.0119793  -0.08666327\n",
            "  0.00464974  0.0111842   0.18678927 -0.1093818  -0.02592649 -0.02197524\n",
            "  0.09157123 -0.06808254  0.01352508  0.00321573  0.05356358  0.10621713\n",
            " -0.00652749 -0.0111255   0.02666534 -0.06686846 -0.0591841  -0.12002304\n",
            "  0.04900586 -0.1372315  -0.09193207 -0.04339473  0.04800321  0.00150247\n",
            "  0.02059186 -0.05222242 -0.1129068   0.10464171  0.08415484  0.01824784\n",
            " -0.00249084 -0.02481512 -0.0506745   0.03895525 -0.03950089  0.02382666\n",
            " -0.03448666  0.09517099 -0.02719707  0.05493838  0.04096664 -0.06114654\n",
            " -0.03877131  0.03147171  0.10843866  0.06421734 -0.05383112  0.10860124\n",
            " -0.05635574 -0.06976023 -0.01608095  0.07269872 -0.06853887  0.0757142\n",
            " -0.06687789 -0.082582    0.03361143 -0.03696954 -0.02082058  0.08681875\n",
            "  0.12874684 -0.06875075 -0.01945563 -0.10659064  0.04690918 -0.07826827\n",
            " -0.01009039 -0.03153319 -0.02015084  0.00530814  0.0300684  -0.05321331\n",
            " -0.03128747 -0.11387593  0.09273294 -0.10927723 -0.07796712 -0.0446558\n",
            " -0.12608154 -0.07060686  0.0797931   0.05716796  0.1500239  -0.01609963\n",
            " -0.02022284 -0.09327734 -0.21885014  0.04208452  0.03770358  0.09872396\n",
            " -0.0584925  -0.03811152  0.01800898 -0.1141545  -0.02953336  0.02136926\n",
            " -0.07371115 -0.06720692  0.11106938  0.0241592  -0.05508298  0.07333101]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.7572609186172485),\n",
              " ('queen', 0.6713276505470276),\n",
              " ('princess', 0.5432624220848083),\n",
              " ('throne', 0.5386104583740234),\n",
              " ('monarch', 0.5347574353218079),\n",
              " ('daughter', 0.498025119304657),\n",
              " ('mother', 0.49564430117607117),\n",
              " ('elizabeth', 0.4832652509212494),\n",
              " ('kingdom', 0.47747087478637695),\n",
              " ('prince', 0.4668239951133728)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def word_analogy_with_vector(model, x_1, x_2, y_1):\n",
        "  '''\n",
        "  This function takes a gensim Word2Vec model and outputs a vector to find y2 that corresponds to x_1 → x_2 == y_1 → y_2\n",
        "  e.g. x_1 (man) → x_2 (king) == y_1 (woman) → y_2(?)\n",
        "\n",
        "  inputs\n",
        "  model (gensim.models.keyedvectors.KeyedVectors): Word2Vec model in KeyedVectors in gensim library\n",
        "  x_1, x_2, y_1 (str): Words in the model's vocabulary.\n",
        "\n",
        "  output (np.ndarray): A vector in np.ndarray, which can be used to find proper y_2 for given (model, x_1, x_2, y_1)\n",
        "\n",
        "  CAUTION: You have to normalize (divide vector by its length) the vector before doing arithmetic.\n",
        "  '''\n",
        "\n",
        "  # Write your code from here\n",
        "\n",
        "  v_x1 = model[x_1] / (sum(model[x_1] ** 2) ** 0.5)\n",
        "  v_x2 = model[x_2] / (sum(model[x_2] ** 2) ** 0.5)\n",
        "  v_y1 = model[y_1] / (sum(model[y_1] ** 2) ** 0.5)\n",
        "\n",
        "  v_y2 = v_y1 + (v_x2 - v_x1)\n",
        "\n",
        "  return v_y2\n",
        "\n",
        "# test whether the function works well\n",
        "result_vector = word_analogy_with_vector(model, 'man', 'king', 'woman')\n",
        "print('result vector is ', result_vector)\n",
        "assert isinstance(result_vector, np.ndarray), \"Output of the function has to be np.ndarray\"\n",
        "model.most_similar(result_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "41604769",
      "metadata": {
        "id": "41604769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062fe436-eb38-49df-ae21-c89dd18188d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6099975687524788\n",
            "gensim library result: 0.6099975\n"
          ]
        }
      ],
      "source": [
        "def get_cosine_similarity(model, x, y):\n",
        "  '''\n",
        "  This function returns cosine similarity of x,y\n",
        "\n",
        "  inputs\n",
        "  model (gensim.models.keyedvectors.KeyedVectors): Word2Vec model in KeyedVectors in gensim library\n",
        "  x, y (str): Words in the model's vocabulary.\n",
        "\n",
        "  output\n",
        "  similarity (float): cosine similarity between x's vector and y's vector\n",
        "  '''\n",
        "  # Write your codes from here\n",
        "\n",
        "  sim = np.dot(model[x], model[y]) / ((sum(model[x] ** 2) ** 0.5) * (sum(model[y] ** 2) ** 0.5))\n",
        "\n",
        "  return sim\n",
        "\n",
        "# test the output with your own choice\n",
        "word_a = 'chocolate'\n",
        "word_b = 'vanilla'\n",
        "\n",
        "similarity = get_cosine_similarity(model, word_a, word_b)\n",
        "print(similarity)\n",
        "assert -1 <= similarity <= 1, \"Similarity has to be between -1 and 1\"\n",
        "\n",
        "print('gensim library result:', model.similarity(word_a, word_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a673d6",
      "metadata": {
        "id": "73a673d6"
      },
      "source": [
        "## Problem 2. Find Most Similar Words\n",
        "- One of the most simple and typical use case of Word2Vec is finding a word based on similarity.\n",
        "- You can list the most similar words for a given query word by using ``model.most_similar(your_word)``\n",
        "    - Usually, every word in Word2Vec model is in lowercase\n",
        "- **In your report**, present more than **5** interesting examples and explain **why it was interesting for you**\n",
        "    - Try to explain why those words are regarded similar in Word2Vec, considering how it was trained\n",
        "- Caution: The model was trained with multilingual dataset. This means the meaning of the word can follow non-English words.\n",
        "    - e.g. \"die\", \"war\" are more frequently used in German than English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ccd292a8",
      "metadata": {
        "id": "ccd292a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35125629-c0a1-4d50-9d5e-b63d31838d6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('radiohead', 0.6285437345504761),\n",
              " ('u2', 0.5711713433265686),\n",
              " ('aerosmith', 0.4990842938423157),\n",
              " ('beyonce', 0.49628856778144836),\n",
              " ('shakira', 0.48919782042503357),\n",
              " ('frontman', 0.4847787022590637),\n",
              " ('björk', 0.47576695680618286),\n",
              " ('minogue', 0.46712526679039),\n",
              " ('rihanna', 0.4642452597618103),\n",
              " ('springsteen', 0.45410770177841187)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "target_word = 'coldplay' # Enter your word string here\n",
        "# check the word is in the vocabulary of the model\n",
        "assert model.has_index_for(target_word), f\"The selected word, {target_word}, is not included in the model's vocabulary\"\n",
        "model.most_similar(target_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c9e1b0",
      "metadata": {
        "id": "43c9e1b0"
      },
      "source": [
        "## Problem 3. Word Analogy\n",
        "- Another interesting thing you can play with Word2Vec is word analogy\n",
        "- Word analogy is done by adding and subtracting the word vector\n",
        "- In the cell below, you can run an example like this\n",
        "    - ``analogy('man', 'king', 'woman')`` represents a question of \"man is to king as woman is to what?\"\n",
        "- **Caution**: Do not confuse the relation between each input word.\n",
        "    - Some wrong examples:\n",
        "      - ``analogy(model, 'student', 'school', 'employee')``  Student: School -> Employee: Teacher?\n",
        "        - This is wrong because the relation between Student and School is not the same as the relation between Employee and Teacher.\n",
        "      - ``analogy(model, 'android', 'electricity', 'blood')``\n",
        "        - This is wrong because it calculates ``electricity - android + blood`` instead of ``android - electricity + blood``\n",
        "- Try with your own choice.\n",
        "- **In your report**, present at least **5** interesting examples of your choice\n",
        "    - You can include the failure case\n",
        "    - Describe what did you expect and why the result was interesting for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b75a2f6a",
      "metadata": {
        "id": "b75a2f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e16ea73-d866-4886-cb0e-740014b5f2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rookie', 0.5814343690872192),\n",
            " ('quarterback', 0.5601049065589905),\n",
            " ('players', 0.5584985613822937),\n",
            " ('starters', 0.5566496253013611),\n",
            " ('lineman', 0.5547109246253967),\n",
            " ('sophomore', 0.5498921275138855),\n",
            " ('scorer', 0.5400611758232117),\n",
            " ('defensive', 0.5371955633163452),\n",
            " ('redshirt', 0.5318215489387512),\n",
            " ('standout', 0.5229174494743347)]\n"
          ]
        }
      ],
      "source": [
        "def analogy(model, x1, x2, y1):\n",
        "  pp.pprint(model.most_similar([x2, y1], negative=[x1]))\n",
        "\n",
        "# Try with your own word choice\n",
        "analogy(model, 'student', 'freshman', 'player') # y1 + (x2 - x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084dc056",
      "metadata": {
        "id": "084dc056"
      },
      "source": [
        "## Problem 4. Visualize Word Vectors\n",
        "- Select a list of words of your interest\n",
        "    - **At least 30 words for minimum**\n",
        "    - ``word_list`` is a list of strings\n",
        "    - every element in ``word_list`` has to be included in the model's vocabulary\n",
        "- Visualize the vectors of words using dimensionality reduction (in this case, PCA)\n",
        "- In your report, describe how words are located in 2D space\n",
        "    - How are the words clustered?\n",
        "    - Do you think the words are properly located based on their semantic meanings?\n",
        "    - Is there anything suprising or unexpected examples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "57c0d085",
      "metadata": {
        "id": "57c0d085"
      },
      "outputs": [],
      "source": [
        "# Run this cell to\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "  if len(words) < 30:\n",
        "    print(\"WARNING: For your report, please select more than 30 word samples for the visualization\")\n",
        "    print(f\"Current length of input word list: {len(words)}\")\n",
        "  word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "  twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "  # plt.figure(figsize=(12,12))\n",
        "  # plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "  # for word, (x,y) in zip(words, twodim):\n",
        "  #     plt.text(x+0.05, y+0.05, word, fontsize=15)\n",
        "  fig = px.scatter(twodim, x=0, y=1, text=words)\n",
        "  fig.update_traces(textposition='top center')\n",
        "  fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7f61a201",
      "metadata": {
        "id": "7f61a201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "add06912-d028-4d70-a2ca-a52f78d255d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: For your report, please select more than 30 word samples for the visualization\n",
            "Current length of input word list: 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"50cf0589-1ec8-4678-ada4-a4f5430c9cf2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"50cf0589-1ec8-4678-ada4-a4f5430c9cf2\")) {                    Plotly.newPlot(                        \"50cf0589-1ec8-4678-ada4-a4f5430c9cf2\",                        [{\"hovertemplate\":\"0=%{x}\\u003cbr\\u003e1=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"guitar\",\"base\",\"drum\",\"keyboard\",\"vocal\",\"bjj\",\"judo\",\"wrestling\",\"boxing\",\"taekwondo\",\"coldplay\",\"oasis\",\"radiohead\",\"keane\",\"maroon5\",\"football\",\"soccer\",\"baseball\",\"basketball\",\"messi\",\"sogang\",\"snu\",\"yonsei\",\"ewha\",\"hongik\"],\"x\":[1.7430533,0.11634547,1.5431767,2.5915954,1.796263,0.39809808,-2.859975,-3.9893448,-3.97523,-2.4494429,3.264357,1.9132922,3.2688327,0.7196654,2.9548054,-4.3866343,-4.1643095,-3.8520865,-4.391023,0.5467276,2.1158502,1.4073502,1.8687314,2.24238,1.5775211],\"xaxis\":\"x\",\"y\":[-3.7551384,-1.4736133,-2.8449183,-2.8891978,-2.474983,0.79789674,1.4811131,-0.50116163,-0.6502758,2.3836405,-2.638474,-1.650721,-2.1566882,-1.3892709,0.43262145,-0.6914054,-0.24173617,-1.0220718,-0.26431888,-0.44330293,3.7335198,2.9304256,4.4535933,5.2864017,3.5880654],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"0\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"1\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('50cf0589-1ec8-4678-ada4-a4f5430c9cf2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Select word list of your own interests\n",
        "word_list = [\n",
        "    'guitar', 'base', 'drum', 'keyboard', 'vocal',\n",
        "    'bjj', 'judo', 'wrestling', 'boxing', 'taekwondo',\n",
        "    'coldplay', 'oasis', 'radiohead', 'keane', 'maroon5',\n",
        "    'football', 'soccer', 'baseball', 'basketball', 'messi',\n",
        "    'sogang', 'snu', 'yonsei', 'ewha', 'hongik'\n",
        "]\n",
        "\n",
        "display_pca_scatterplot(model, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7130b706",
      "metadata": {
        "id": "7130b706"
      },
      "source": [
        "## Problem 5. Train New Word2Vec\n",
        "- Word2Vec models can be trained on different corpus (text)\n",
        "- Train your own model with your custom selection of text\n",
        "- In your report, present at least **5** interesting examples that makes different result by dataset selection\n",
        "    - You can compare some word analogy examples or similairites or visualization\n",
        "    - You don't have to repeat all the analysis again. Select some examples that you think are interesting\n",
        "- Explain the difference of the result by dataset selection\n",
        "- You can refer [Official Documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "f5ef0a7f",
      "metadata": {
        "id": "f5ef0a7f"
      },
      "outputs": [],
      "source": [
        "# You don't have to change this cell\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def remove_punctuation(x):\n",
        "  return x.translate(''.maketrans('', '', string.punctuation))\n",
        "def make_tokenized_corpus(corpus):\n",
        "  out= [ [y.lower() for y in remove_punctuation(sentence).split(' ') if y] for sentence in corpus]\n",
        "  return [x for x in out if x!=[]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88b731a",
      "metadata": {
        "id": "e88b731a"
      },
      "outputs": [],
      "source": [
        "your_text_fn = '' # Enter your text file name here\n",
        "\n",
        "with open(your_text_fn, 'r') as f:\n",
        "  strings = f.readlines()\n",
        "\n",
        "'''\n",
        "This line is for the case when the text file is not properly formatted.\n",
        "It was used to ignore linebreaks and join the sentences into one string, since the text example included linebreak following printed book lines.\n",
        "\n",
        "strings = \"\".join(strings).replace('\\n', ' ').replace('Mr.', 'mr').replace('Mrs.', 'mrs').split('. ')\n",
        "'''\n",
        "# The strings has to be a list of list of strings, where inner list is a sentence\n",
        "\n",
        "print(\"Checking the first 5 sentences in the text file\")\n",
        "for i in range(5):\n",
        "  print(f\"Sentence {i+1}: {strings[i]}\")\n",
        "corpus = make_tokenized_corpus(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c8153b",
      "metadata": {
        "id": "e6c8153b"
      },
      "source": [
        "- gensim Word2Vec arguments\n",
        "  - ``sentences``: list of list of strings\n",
        "  - ``vector_size``: dimension of word vector\n",
        "  - ``epochs``: number of epoch to train the word2vec model\n",
        "  - ``window``: maximum distance between the current and predicted word within a sentence\n",
        "  - ``min_count``: ignore all words with total frequency lower than this\n",
        "  - ``sg``: training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "  - ``negative``: if > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown (usually between 5-20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b994d0f",
      "metadata": {
        "id": "2b994d0f"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(sentences=corpus, vector_size=200, window=5, min_count=2, epochs=50, sg=1)\n",
        "model = model.wv # To match with previous codes, we use wv (KeyedVector) of the Word2Vec class\n",
        "# Try the function above with the newely trained model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}