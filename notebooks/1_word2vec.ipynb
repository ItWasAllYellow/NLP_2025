{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItWasAllYellow/NLP_2025/blob/main/notebooks/1_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FZ2ZSwcyhDQ"
      },
      "source": [
        "# Word2Vec Implementation from Scratch\n",
        "\n",
        "This notebook demonstrates how to implement the Word2Vec algorithm from scratch using PyTorch. We'll use the first Harry Potter book as our corpus to train word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDRbSHQIyhDS"
      },
      "source": [
        "## 1. Setting Up the Environment\n",
        "\n",
        "First, we need to import the necessary libraries:\n",
        "- `torch` and `torch.nn` for tensor operations and neural network functionality\n",
        "- `string` for string manipulations (removing punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "L9BA5Lg2QRMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA7ERi6CyhDS"
      },
      "source": [
        "## 2. Getting the Text Data\n",
        "\n",
        "We'll download the first Harry Potter book to use as our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEaaz_s0QRMs",
        "outputId": "dbb54985-694c-42ac-e2ab-83099aa0c548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-25 05:02:25--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt.1’\n",
            "\n",
            "\r          J. K. Row   0%[                    ]       0  --.-KB/s               \rJ. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-25 05:02:26 (9.15 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt.1’ saved [439742/439742]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6qumaDyhDT"
      },
      "source": [
        "## 3. Text Preprocessing\n",
        "\n",
        "Before we can use the text data, we need to preprocess it:\n",
        "- Remove punctuation\n",
        "- Convert text to lowercase\n",
        "- Split text into tokens (words)\n",
        "\n",
        "This function will help us clean and tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "CUsXJYlIQRMs"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(x):\n",
        "  return x.translate(''.maketrans('', '', string.punctuation))\n",
        "\n",
        "def make_tokenized_corpus(corpus):\n",
        "  out= [ [y.lower() for y in remove_punctuation(sentence).split(' ') if y] for sentence in corpus]\n",
        "  return [x for x in out if x!=[]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptv8VRGXyhDT"
      },
      "source": [
        "## 4. Loading and Formatting the Text\n",
        "\n",
        "Now we'll load the text file, replace some special characters, and split the text into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Ry1o-F-bQRMs"
      },
      "outputs": [],
      "source": [
        "with open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'r') as f:\n",
        "  strings = f.readlines()\n",
        "sample_text = \"\".join(strings).replace('\\n', ' ').replace('Mr.', 'mr').replace('Mrs.', 'mrs').split('. ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-mCXyJuyhDT"
      },
      "source": [
        "Let's tokenize the text using our preprocessing function `make_tokenized_corpus`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Ne-pUaxSQRMs"
      },
      "outputs": [],
      "source": [
        "# Corpus is a list of list of strings (words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = make_tokenized_corpus(sample_text)\n",
        "\n",
        "corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx6UoEHjylS1",
        "outputId": "c067b7ae-a303-412c-f617-f3a985a1806d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['harry',\n",
              "  'potter',\n",
              "  'and',\n",
              "  'the',\n",
              "  'sorcerers',\n",
              "  'stone',\n",
              "  'chapter',\n",
              "  'one',\n",
              "  'the',\n",
              "  'boy',\n",
              "  'who',\n",
              "  'lived',\n",
              "  'mr',\n",
              "  'and',\n",
              "  'mrs',\n",
              "  'dursley',\n",
              "  'of',\n",
              "  'number',\n",
              "  'four',\n",
              "  'privet',\n",
              "  'drive',\n",
              "  'were',\n",
              "  'proud',\n",
              "  'to',\n",
              "  'say',\n",
              "  'that',\n",
              "  'they',\n",
              "  'were',\n",
              "  'perfectly',\n",
              "  'normal',\n",
              "  'thank',\n",
              "  'you',\n",
              "  'very',\n",
              "  'much'],\n",
              " ['they',\n",
              "  'were',\n",
              "  'the',\n",
              "  'last',\n",
              "  'people',\n",
              "  'youd',\n",
              "  'expect',\n",
              "  'to',\n",
              "  'be',\n",
              "  'involved',\n",
              "  'in',\n",
              "  'anything',\n",
              "  'strange',\n",
              "  'or',\n",
              "  'mysterious',\n",
              "  'because',\n",
              "  'they',\n",
              "  'just',\n",
              "  'didnt',\n",
              "  'hold',\n",
              "  'with',\n",
              "  'such',\n",
              "  'nonsense'],\n",
              " ['mr',\n",
              "  'dursley',\n",
              "  'was',\n",
              "  'the',\n",
              "  'director',\n",
              "  'of',\n",
              "  'a',\n",
              "  'firm',\n",
              "  'called',\n",
              "  'grunnings',\n",
              "  'which',\n",
              "  'made',\n",
              "  'drills'],\n",
              " ['he',\n",
              "  'was',\n",
              "  'a',\n",
              "  'big',\n",
              "  'beefy',\n",
              "  'man',\n",
              "  'with',\n",
              "  'hardly',\n",
              "  'any',\n",
              "  'neck',\n",
              "  'although',\n",
              "  'he',\n",
              "  'did',\n",
              "  'have',\n",
              "  'a',\n",
              "  'very',\n",
              "  'large',\n",
              "  'mustache'],\n",
              " ['mrs',\n",
              "  'dursley',\n",
              "  'was',\n",
              "  'thin',\n",
              "  'and',\n",
              "  'blonde',\n",
              "  'and',\n",
              "  'had',\n",
              "  'nearly',\n",
              "  'twice',\n",
              "  'the',\n",
              "  'usual',\n",
              "  'amount',\n",
              "  'of',\n",
              "  'neck',\n",
              "  'which',\n",
              "  'came',\n",
              "  'in',\n",
              "  'very',\n",
              "  'useful',\n",
              "  'as',\n",
              "  'she',\n",
              "  'spent',\n",
              "  'so',\n",
              "  'much',\n",
              "  'of',\n",
              "  'her',\n",
              "  'time',\n",
              "  'craning',\n",
              "  'over',\n",
              "  'garden',\n",
              "  'fences',\n",
              "  'spying',\n",
              "  'on',\n",
              "  'the',\n",
              "  'neighbors']]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLBY2P7_yhDU"
      },
      "source": [
        "## 5. Creating Context Word Pairs\n",
        "\n",
        "A key concept in Word2Vec is learning from context. We need to create pairs of words that appear near each other in the text. We'll use a sliding window approach to create these pairs.\n",
        "\n",
        "For example, with the window size of 2, for the word \"to\" in the sentence \"they were the last people youd expect to be involved...\", we would create pairs with:\n",
        "- (\"to\", \"expect\")\n",
        "- (\"to\", \"be\")\n",
        "- (\"to\", \"involved\")\n",
        "- (\"to\", \"in\")\n",
        "\n",
        "These pairs will be our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UxJwTAacWfP",
        "outputId": "9072ec09-f94b-4ba3-d594-6c0f6c9c0146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4682/4682 [00:00<00:00, 15490.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Length of word_pairs is 282372\n",
            "First 5 example of word_pairs is [('harry', 'potter'), ('harry', 'and'), ('potter', 'harry'), ('potter', 'and'), ('potter', 'the')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "sample_sentence = ['they', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense']\n",
        "\n",
        "word_pairs = []\n",
        "window_size = 2\n",
        "\n",
        "for sample_sentence in tqdm(corpus):\n",
        "  for cur_idx, center_word in enumerate(sample_sentence):\n",
        "    window_begin = max(cur_idx - window_size, 0)\n",
        "    window_end = min(cur_idx + window_size + 1, len(sample_sentence))\n",
        "    # for context_word in sample_sentence[window_begin:window_end]:\n",
        "    #   # if center_word == context_word: continue\n",
        "    #   word_pairs.append( (center_word, context_word))\n",
        "    for j in range(window_begin, window_end):\n",
        "      if cur_idx == j: continue\n",
        "      word_pairs.append( (center_word, sample_sentence[j]))\n",
        "\n",
        "print(f\"\\nLength of word_pairs is {len(word_pairs)}\")\n",
        "print(f\"First 5 example of word_pairs is {word_pairs[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD-SN32cyhDU"
      },
      "source": [
        "## 6. Building the Vocabulary\n",
        "\n",
        "To work with word vectors, we need to create a vocabulary that maps each unique word to an index. We'll also filter out rare words that appear less than a certain number of times in the corpus.\n",
        "\n",
        "### 6.1 Collecting All Words\n",
        "\n",
        "First, let's collect all words in our corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "htinJiMPkkRE"
      },
      "outputs": [],
      "source": [
        "# we have to make vocabulary\n",
        "sentence = corpus[0]\n",
        "entire_words = []\n",
        "\n",
        "for sentence in corpus:\n",
        "  for word in sentence:\n",
        "    entire_words.append(word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_68LD6IyhDU"
      },
      "source": [
        "\n",
        "### 6.2 Finding Unique Words\n",
        "\n",
        "Now, let's find the unique words in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBFCjeslgDe",
        "outputId": "fac08fc2-b2f4-4c88-d5d4-8d2d3aa9aa84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6038"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# we have to get the \"unique\" item among total words\n",
        "\n",
        "unique_words = set(entire_words)\n",
        "len(unique_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7oqHw4dyhDU"
      },
      "source": [
        "### 6.3 Converting to a List and Sorting\n",
        "\n",
        "We'll convert the set of unique words to a sorted list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fDJNrHdhl_dk",
        "outputId": "54facd8c-b837-4ced-8914-317b7c822239"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# vocab_set[0] # set is not subscriptable because it has no order\n",
        "\n",
        "unique_words = sorted(list(unique_words))\n",
        "unique_words[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPBBsJCyhDU"
      },
      "source": [
        "### 6.4 Filtering by Frequency\n",
        "\n",
        "Now, let's filter out rare words that occur less than a specified number of times:\n",
        "- We can use the `Counter` class from the `collections` module to count the frequency of each word in the corpus.\n",
        "- Caution on `alist.sort()` will return `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOkBSjrkmNE4",
        "outputId": "604c3031-d7d9-42a3-8073-cb43be62f1c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'able',\n",
              " 'abou',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'added',\n",
              " 'afford',\n",
              " 'afraid',\n",
              " 'after']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# how can we filter the vocab by its frequency?\n",
        "filtered_vocab = None\n",
        "# you can use word counter as dictionary\n",
        "# In python dictionary, dict.keys() gives keys, and dict.values() give values,\n",
        "# dict.items() give (key, value)\n",
        "\n",
        "from collections import Counter\n",
        "word_counter = Counter(entire_words)\n",
        "word_counter.most_common(10)\n",
        "word_counter['harry']\n",
        "\n",
        "threshold = 5\n",
        "filtered_vocab = []\n",
        "\n",
        "for key, value in word_counter.items():\n",
        "  if value > threshold:\n",
        "    filtered_vocab.append(key)\n",
        "\n",
        "filtered_vocab.sort()\n",
        "filtered_vocab[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfez7UfYyhDU"
      },
      "source": [
        "## 7. Filtering Word Pairs\n",
        "\n",
        "Now that we have our filtered vocabulary, we need to filter our word pairs to only include words that are in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUS6U7y7opUp",
        "outputId": "e5e8ec72-234f-47c8-8a45-22807954adc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 282372/282372 [00:00<00:00, 965815.94it/s] \n"
          ]
        }
      ],
      "source": [
        "# Filter the word_pairs using the vocab\n",
        "# word_pairs, filtered_vocab\n",
        "# word_pairs is a list of [word_a, word_b]\n",
        "\n",
        "filtered_word_pairs = []\n",
        "vocab_set = set(filtered_vocab)\n",
        "\n",
        "for pair in tqdm(word_pairs):\n",
        "  a, b = pair\n",
        "  if a in vocab_set and b in vocab_set:\n",
        "    filtered_word_pairs.append(pair)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "U-o4UucOrcem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63444c7-31a1-43d0-c036-4954e02185c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('harry', 'potter')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# implement same algorithm with list comprehension\n",
        "\n",
        "filtered_word_pairs = [pair for pair in word_pairs if pair[0] in vocab_set and pair[1] in vocab_set]\n",
        "filtered_word_pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_8ch59ps_P",
        "outputId": "42028dbd-64d6-4a5d-fcaf-03b33bcb735f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226846, 282372)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "len(filtered_word_pairs), len(word_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqUhV1ayhDU"
      },
      "source": [
        "## 8. Converting Words to Indices\n",
        "\n",
        "For efficiency, we'll convert our words to indices according to their position in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFJfhOznqyi-",
        "outputId": "1a7d273f-4429-42d1-99eb-ccab360198cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# convert word into index of vocab\n",
        "filtered_vocab.index('harry')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7Ov52nyhDU"
      },
      "source": [
        "This is inefficient because `list.index()` has to scan the list every time. Let's use a dictionary for faster lookups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D8n16VitIHP",
        "outputId": "2ea0befc-10f8-406d-b087-3d57520620a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# we can make it faster\n",
        "# use dictionary to find the index of string\n",
        "word2idx = dict()\n",
        "for idx, word in enumerate(filtered_vocab):\n",
        "  word2idx[word] = idx\n",
        "word2idx['harry']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9ESVqLyhDV"
      },
      "source": [
        "Now, let's convert our word pairs to index pairs more efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utXemuOgt8-o",
        "outputId": "e38950f9-8e02-4b22-b9ed-4cbdb096a2ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(527, 953)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "index_pairs = [(word2idx[pair[0]], word2idx[pair[1]]) for pair in filtered_word_pairs]\n",
        "index_pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KgQ_oSNGuZAd",
        "outputId": "86a4bdfb-f06a-4819-abba-530a055af469"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'harry'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# Why we don't need idx2tok?\n",
        "\n",
        "filtered_vocab[527]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waHqgJANyhDV"
      },
      "source": [
        "## 9. Creating Initial Word Vectors\n",
        "\n",
        "Now we'll create random vectors for each word in our vocabulary. These vectors will be adjusted during training:\n",
        "- We can use `torch.randn` to create random vectors that follow normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygV93qzDu4Ls",
        "outputId": "e46cd00a-0aa3-4155-8b54-68af6aadba32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1157,  0.0247,  0.0972,  ..., -0.1338,  0.0022, -0.1012],\n",
              "        [-0.0096, -0.0748, -0.1745,  ..., -0.1341, -0.0008,  0.0204],\n",
              "        [-0.1464,  0.0044, -0.0773,  ...,  0.1289, -0.0269,  0.0018],\n",
              "        ...,\n",
              "        [-0.0651, -0.0527, -0.0294,  ...,  0.0104, -0.0203,  0.1157],\n",
              "        [-0.0653,  0.0210,  0.0966,  ..., -0.0534,  0.0205,  0.0437],\n",
              "        [-0.1655,  0.1122,  0.0916,  ...,  0.0498,  0.0019, -0.0153]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# we have to make random vectors for each word in the vocab\n",
        "# we also have to decide the dimension of the vector\n",
        "\n",
        "dim = 100\n",
        "vocab_size = len(filtered_vocab)\n",
        "\n",
        "word_vectors = torch.randn(vocab_size, dim) / dim ** 0.5\n",
        "word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmZcT53rvwW2",
        "outputId": "1f425cfb-9b0c-4f22-869e-1dad4d2e875b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0504, -0.0973,  0.0049, -0.0971, -0.1081, -0.0207, -0.1326,  0.1066,\n",
              "         0.0075, -0.1111,  0.0222, -0.0561,  0.1361, -0.1272, -0.0758, -0.0151,\n",
              "        -0.0783,  0.0760, -0.0923,  0.0695,  0.1304,  0.0105, -0.0271,  0.2367,\n",
              "        -0.1614,  0.0124,  0.0277, -0.1525,  0.0538, -0.0022,  0.0950, -0.0174,\n",
              "         0.0402, -0.0209, -0.0667, -0.0365,  0.0713, -0.0438,  0.0122, -0.0124,\n",
              "         0.1244, -0.0296, -0.2213,  0.1383, -0.0390,  0.0275, -0.0977, -0.1840,\n",
              "        -0.0530,  0.0767,  0.1528, -0.0832,  0.1431,  0.1079,  0.2552,  0.0963,\n",
              "         0.2991,  0.1307,  0.0355, -0.1131, -0.1334,  0.0028,  0.0643,  0.0495,\n",
              "         0.0598,  0.1758,  0.0314,  0.0667, -0.1023, -0.0904, -0.0911, -0.0589,\n",
              "        -0.0634, -0.0363,  0.1372,  0.0720,  0.0369, -0.2115,  0.0713, -0.0186,\n",
              "        -0.1320, -0.0541,  0.0791,  0.0315,  0.1653,  0.1592, -0.1044,  0.0137,\n",
              "        -0.1315,  0.0473, -0.0619,  0.0229,  0.1658, -0.0618, -0.0979, -0.0376,\n",
              "         0.1491,  0.0391, -0.1021,  0.0140])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# what is the vector for harry?\n",
        "\n",
        "word_vectors[word2idx['harry']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLdvJrdKyhDV"
      },
      "source": [
        "## 10. Understanding Word Relationships with Dot Products\n",
        "\n",
        "The core of Word2Vec is using dot products to measure relationships between words. Let's explore this concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "J4bEKFvUxbVv"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(sci_mode=False) # Do this to avoid scientific notation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCMWmD6Suhy"
      },
      "source": [
        "## Dot Product\n",
        "- Assume we have two vectors $a$ and $b$.\n",
        "  - $a = [a_1, a_2, a_3, a_4, ..., a_n]$\n",
        "  - $b = [b_1, b_2, b_3, b_4, ..., b_n]$\n",
        "- $a \\cdot b$ = $\\sum _{i=1}^n a_ib_i$  = $a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + ... + a_nb_n$\n",
        "\n",
        "Let's calculate the dot product between \"harry\" and \"potter\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsse-jUrw6c2",
        "outputId": "514fe10a-fb58-4e09-d04f-a1fba8e12de0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0024)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# calculate P(potter|harry)\n",
        "harry = word_vectors[word2idx['harry']]\n",
        "potter = word_vectors[word2idx['potter']]\n",
        "dot_product_value_between_potter_harry = sum(harry * potter)\n",
        "dot_product_value_between_potter_harry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZrLEY36yBNZ",
        "outputId": "85692e8e-4fe7-4b67-cc08-14510b49a121",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': tensor(0.0264),\n",
              " 'able': tensor(0.0354),\n",
              " 'abou': tensor(-0.0322),\n",
              " 'about': tensor(0.1471),\n",
              " 'above': tensor(-0.0653),\n",
              " 'across': tensor(-0.2437),\n",
              " 'added': tensor(-0.0568),\n",
              " 'afford': tensor(0.0984),\n",
              " 'afraid': tensor(-0.1479),\n",
              " 'after': tensor(0.0427),\n",
              " 'afternoon': tensor(-0.2207),\n",
              " 'again': tensor(0.1611),\n",
              " 'against': tensor(-0.1475),\n",
              " 'ages': tensor(-0.1289),\n",
              " 'ago': tensor(-0.0539),\n",
              " 'agreed': tensor(0.0839),\n",
              " 'ah': tensor(0.1889),\n",
              " 'ahead': tensor(-0.0655),\n",
              " 'air': tensor(0.0361),\n",
              " 'albus': tensor(0.2263),\n",
              " 'alive': tensor(0.0017),\n",
              " 'all': tensor(-0.0815),\n",
              " 'alley': tensor(-0.0512),\n",
              " 'allowed': tensor(-0.1342),\n",
              " 'almost': tensor(-0.0179),\n",
              " 'alone': tensor(-0.0028),\n",
              " 'along': tensor(0.0790),\n",
              " 'already': tensor(-0.1897),\n",
              " 'also': tensor(0.0429),\n",
              " 'although': tensor(-0.0778),\n",
              " 'always': tensor(-0.0673),\n",
              " 'am': tensor(0.1385),\n",
              " 'an': tensor(-0.0567),\n",
              " 'and': tensor(-0.0139),\n",
              " 'angrily': tensor(-0.1191),\n",
              " 'angry': tensor(0.1273),\n",
              " 'another': tensor(0.0842),\n",
              " 'answer': tensor(0.0649),\n",
              " 'any': tensor(0.0401),\n",
              " 'anymore': tensor(-0.1393),\n",
              " 'anyone': tensor(0.0243),\n",
              " 'anythin': tensor(-0.0862),\n",
              " 'anything': tensor(0.0485),\n",
              " 'anyway': tensor(-0.1482),\n",
              " 'anywhere': tensor(-0.0328),\n",
              " 'apart': tensor(-0.2359),\n",
              " 'appeared': tensor(0.1020),\n",
              " 'are': tensor(0.0156),\n",
              " 'arent': tensor(-0.0063),\n",
              " 'arm': tensor(-0.0029),\n",
              " 'armor': tensor(-0.1427),\n",
              " 'arms': tensor(-0.1229),\n",
              " 'around': tensor(-0.0136),\n",
              " 'arrived': tensor(-0.0257),\n",
              " 'arts': tensor(-0.1841),\n",
              " 'as': tensor(-0.0167),\n",
              " 'ask': tensor(-0.0195),\n",
              " 'asked': tensor(-0.0497),\n",
              " 'asking': tensor(0.0463),\n",
              " 'asleep': tensor(-0.1040),\n",
              " 'at': tensor(0.0278),\n",
              " 'attention': tensor(-0.0636),\n",
              " 'aunt': tensor(-0.0297),\n",
              " 'awake': tensor(0.0576),\n",
              " 'away': tensor(-0.0732),\n",
              " 'baby': tensor(-0.0201),\n",
              " 'back': tensor(0.1909),\n",
              " 'backward': tensor(0.1231),\n",
              " 'bacon': tensor(-0.0651),\n",
              " 'bad': tensor(0.0736),\n",
              " 'bag': tensor(-0.0250),\n",
              " 'ball': tensor(-0.0018),\n",
              " 'balls': tensor(-0.0664),\n",
              " 'bane': tensor(0.0707),\n",
              " 'barrier': tensor(-0.0838),\n",
              " 'be': tensor(0.1155),\n",
              " 'beans': tensor(    -0.0000),\n",
              " 'beard': tensor(-0.0148),\n",
              " 'became': tensor(0.0252),\n",
              " 'because': tensor(0.0056),\n",
              " 'become': tensor(0.0922),\n",
              " 'bed': tensor(0.0753),\n",
              " 'bedroom': tensor(0.0178),\n",
              " 'been': tensor(-0.1660),\n",
              " 'before': tensor(-0.1102),\n",
              " 'began': tensor(-0.1480),\n",
              " 'behind': tensor(-0.0028),\n",
              " 'being': tensor(0.0410),\n",
              " 'believe': tensor(-0.1389),\n",
              " 'below': tensor(-0.1192),\n",
              " 'beneath': tensor(0.0101),\n",
              " 'bent': tensor(0.1602),\n",
              " 'best': tensor(-0.0763),\n",
              " 'bet': tensor(0.1319),\n",
              " 'better': tensor(-0.0828),\n",
              " 'between': tensor(-0.0045),\n",
              " 'big': tensor(0.1657),\n",
              " 'bill': tensor(-0.0012),\n",
              " 'bin': tensor(0.0991),\n",
              " 'binoculars': tensor(0.0358),\n",
              " 'birthday': tensor(-0.0290),\n",
              " 'bit': tensor(-0.0193),\n",
              " 'black': tensor(0.0533),\n",
              " 'blankets': tensor(-0.0348),\n",
              " 'blew': tensor(-0.0911),\n",
              " 'blood': tensor(-0.0837),\n",
              " 'bloody': tensor(0.0713),\n",
              " 'bludger': tensor(-0.0189),\n",
              " 'bludgers': tensor(0.0166),\n",
              " 'blue': tensor(0.1320),\n",
              " 'board': tensor(0.1334),\n",
              " 'boat': tensor(-0.0368),\n",
              " 'boats': tensor(0.0860),\n",
              " 'body': tensor(0.0157),\n",
              " 'book': tensor(0.1344),\n",
              " 'books': tensor(-0.0134),\n",
              " 'both': tensor(-0.0230),\n",
              " 'bottle': tensor(0.0295),\n",
              " 'bottles': tensor(-0.0173),\n",
              " 'bottom': tensor(-0.1953),\n",
              " 'bought': tensor(0.0260),\n",
              " 'bowed': tensor(0.0415),\n",
              " 'box': tensor(-0.1273),\n",
              " 'boy': tensor(-0.0489),\n",
              " 'boys': tensor(-0.0235),\n",
              " 'branches': tensor(0.0669),\n",
              " 'brave': tensor(0.0016),\n",
              " 'break': tensor(-0.0917),\n",
              " 'breakfast': tensor(0.0373),\n",
              " 'breaking': tensor(-0.0711),\n",
              " 'breath': tensor(0.0265),\n",
              " 'breathing': tensor(-0.0019),\n",
              " 'bright': tensor(-0.0523),\n",
              " 'brilliant': tensor(0.2502),\n",
              " 'broke': tensor(0.0065),\n",
              " 'broken': tensor(-0.0803),\n",
              " 'broom': tensor(-0.1414),\n",
              " 'brooms': tensor(-0.1048),\n",
              " 'broomstick': tensor(-0.0988),\n",
              " 'broomsticks': tensor(-0.0988),\n",
              " 'brother': tensor(0.1428),\n",
              " 'brothers': tensor(0.0032),\n",
              " 'brought': tensor(0.1143),\n",
              " 'brown': tensor(-0.0875),\n",
              " 'burst': tensor(-0.1087),\n",
              " 'business': tensor(0.0845),\n",
              " 'busy': tensor(-0.0234),\n",
              " 'but': tensor(0.0008),\n",
              " 'buy': tensor(-0.0249),\n",
              " 'by': tensor(0.1505),\n",
              " 'cake': tensor(0.0299),\n",
              " 'cakes': tensor(-0.1337),\n",
              " 'call': tensor(0.0383),\n",
              " 'called': tensor(0.1023),\n",
              " 'came': tensor(0.0798),\n",
              " 'can': tensor(0.0363),\n",
              " 'cant': tensor(-0.1595),\n",
              " 'car': tensor(-0.0552),\n",
              " 'card': tensor(-0.0389),\n",
              " 'care': tensor(-0.0914),\n",
              " 'careful': tensor(0.0823),\n",
              " 'carefully': tensor(0.1106),\n",
              " 'carried': tensor(-0.0857),\n",
              " 'carrying': tensor(0.0367),\n",
              " 'cart': tensor(0.0016),\n",
              " 'case': tensor(-0.1180),\n",
              " 'castle': tensor(-0.0871),\n",
              " 'cat': tensor(0.0050),\n",
              " 'catch': tensor(0.0350),\n",
              " 'cats': tensor(-0.1365),\n",
              " 'caught': tensor(-0.0677),\n",
              " 'cauldron': tensor(0.0137),\n",
              " 'cause': tensor(-0.0619),\n",
              " 'ceiling': tensor(0.1580),\n",
              " 'centaur': tensor(-0.0951),\n",
              " 'certainly': tensor(0.0685),\n",
              " 'chair': tensor(0.0942),\n",
              " 'chamber': tensor(0.1166),\n",
              " 'chance': tensor(0.0730),\n",
              " 'change': tensor(0.0168),\n",
              " 'changed': tensor(0.1081),\n",
              " 'chapter': tensor(    -0.0001),\n",
              " 'charlie': tensor(-0.0532),\n",
              " 'charlies': tensor(-0.0958),\n",
              " 'charms': tensor(0.0634),\n",
              " 'chasers': tensor(-0.0576),\n",
              " 'cheer': tensor(-0.1570),\n",
              " 'cheering': tensor(0.0016),\n",
              " 'cheers': tensor(-0.0145),\n",
              " 'chess': tensor(-0.0158),\n",
              " 'chessmen': tensor(0.1003),\n",
              " 'chest': tensor(-0.0305),\n",
              " 'chocolate': tensor(0.0416),\n",
              " 'christmas': tensor(-0.1121),\n",
              " 'chuckled': tensor(0.0765),\n",
              " 'clambered': tensor(0.0538),\n",
              " 'clapped': tensor(0.1015),\n",
              " 'class': tensor(-0.1425),\n",
              " 'classes': tensor(0.0271),\n",
              " 'classroom': tensor(-0.0493),\n",
              " 'clean': tensor(-0.0182),\n",
              " 'clear': tensor(-0.2572),\n",
              " 'cleared': tensor(-0.0642),\n",
              " 'clearing': tensor(0.0781),\n",
              " 'clearly': tensor(0.0438),\n",
              " 'clicked': tensor(-0.0453),\n",
              " 'climbed': tensor(0.0797),\n",
              " 'cloak': tensor(0.1241),\n",
              " 'close': tensor(-0.1766),\n",
              " 'closer': tensor(-0.1212),\n",
              " 'clothes': tensor(0.0810),\n",
              " 'club': tensor(0.1322),\n",
              " 'clutching': tensor(-0.0053),\n",
              " 'coat': tensor(-0.0031),\n",
              " 'cold': tensor(-0.1071),\n",
              " 'come': tensor(-0.0813),\n",
              " 'coming': tensor(0.0967),\n",
              " 'common': tensor(0.0600),\n",
              " 'compartment': tensor(0.1298),\n",
              " 'completely': tensor(0.0322),\n",
              " 'computer': tensor(-0.1646),\n",
              " 'control': tensor(-0.0287),\n",
              " 'corner': tensor(0.1001),\n",
              " 'corridor': tensor(0.1300),\n",
              " 'corridors': tensor(-0.1307),\n",
              " 'could': tensor(0.0934),\n",
              " 'couldnt': tensor(0.0325),\n",
              " 'couple': tensor(-0.1500),\n",
              " 'courage': tensor(0.1276),\n",
              " 'course': tensor(-0.1018),\n",
              " 'covered': tensor(-0.1059),\n",
              " 'crabbe': tensor(-0.2386),\n",
              " 'crack': tensor(0.0577),\n",
              " 'crash': tensor(0.0348),\n",
              " 'crate': tensor(-0.0867),\n",
              " 'crept': tensor(0.0113),\n",
              " 'cried': tensor(0.0897),\n",
              " 'cross': tensor(0.0297),\n",
              " 'crossed': tensor(0.0243),\n",
              " 'crowd': tensor(0.1805),\n",
              " 'cry': tensor(0.1961),\n",
              " 'crying': tensor(-0.0874),\n",
              " 'cup': tensor(0.0498),\n",
              " 'cupboard': tensor(0.0877),\n",
              " 'curious': tensor(0.0374),\n",
              " 'curse': tensor(0.0360),\n",
              " 'cut': tensor(0.0547),\n",
              " 'dad': tensor(0.0158),\n",
              " 'damp': tensor(0.0120),\n",
              " 'dangerous': tensor(0.0618),\n",
              " 'dare': tensor(-0.1629),\n",
              " 'dark': tensor(0.1159),\n",
              " 'darkly': tensor(0.1865),\n",
              " 'darkness': tensor(-0.0203),\n",
              " 'day': tensor(0.0709),\n",
              " 'days': tensor(0.0395),\n",
              " 'dead': tensor(-0.1531),\n",
              " 'dean': tensor(-0.0454),\n",
              " 'dear': tensor(0.0836),\n",
              " 'death': tensor(-0.0197),\n",
              " 'decided': tensor(-0.0147),\n",
              " 'deep': tensor(0.1375),\n",
              " 'delighted': tensor(0.1260),\n",
              " 'desk': tensor(0.0366),\n",
              " 'desperate': tensor(0.1149),\n",
              " 'diagon': tensor(0.3247),\n",
              " 'did': tensor(0.1160),\n",
              " 'didnt': tensor(-0.0181),\n",
              " 'die': tensor(0.0285),\n",
              " 'died': tensor(0.0154),\n",
              " 'difference': tensor(-0.1284),\n",
              " 'different': tensor(-0.0051),\n",
              " 'difficult': tensor(0.0006),\n",
              " 'dinner': tensor(0.0974),\n",
              " 'direction': tensor(-0.1697),\n",
              " 'disappeared': tensor(-0.1504),\n",
              " 'dived': tensor(0.0506),\n",
              " 'do': tensor(0.2076),\n",
              " 'does': tensor(-0.0382),\n",
              " 'doesnt': tensor(-0.0270),\n",
              " 'dog': tensor(0.1526),\n",
              " 'dogs': tensor(-0.0315),\n",
              " 'doing': tensor(0.1631),\n",
              " 'don': tensor(-0.0234),\n",
              " 'done': tensor(-0.1020),\n",
              " 'dont': tensor(0.0171),\n",
              " 'door': tensor(-0.1606),\n",
              " 'doors': tensor(-0.0860),\n",
              " 'doorway': tensor(-0.0713),\n",
              " 'dormitory': tensor(0.0820),\n",
              " 'down': tensor(0.0519),\n",
              " 'draco': tensor(0.0932),\n",
              " 'dragged': tensor(0.0235),\n",
              " 'dragon': tensor(0.0807),\n",
              " 'dragons': tensor(-0.0307),\n",
              " 'dream': tensor(0.1724),\n",
              " 'dressed': tensor(0.0077),\n",
              " 'drew': tensor(-0.0029),\n",
              " 'drills': tensor(-0.0564),\n",
              " 'drink': tensor(0.1530),\n",
              " 'drive': tensor(0.0437),\n",
              " 'drop': tensor(0.0387),\n",
              " 'dropped': tensor(-0.1287),\n",
              " 'drove': tensor(0.0155),\n",
              " 'dudley': tensor(-0.0357),\n",
              " 'dudleys': tensor(0.0296),\n",
              " 'dumbledore': tensor(0.2381),\n",
              " 'dumbledores': tensor(0.1522),\n",
              " 'dungeons': tensor(-0.0845),\n",
              " 'dunno': tensor(0.1365),\n",
              " 'during': tensor(0.1185),\n",
              " 'dursley': tensor(-0.0498),\n",
              " 'dursleys': tensor(-0.2214),\n",
              " 'each': tensor(-0.1449),\n",
              " 'eagerly': tensor(-0.0296),\n",
              " 'ear': tensor(0.2172),\n",
              " 'ears': tensor(0.0484),\n",
              " 'earth': tensor(0.0905),\n",
              " 'easily': tensor(-0.0957),\n",
              " 'easy': tensor(0.0444),\n",
              " 'eat': tensor(-0.2318),\n",
              " 'eating': tensor(-0.2890),\n",
              " 'edge': tensor(-0.1475),\n",
              " 'egg': tensor(-0.1948),\n",
              " 'eh': tensor(0.0463),\n",
              " 'either': tensor(-0.1285),\n",
              " 'eleven': tensor(0.1286),\n",
              " 'else': tensor(-0.0301),\n",
              " 'em': tensor(-0.1032),\n",
              " 'empty': tensor(0.0426),\n",
              " 'end': tensor(-0.0404),\n",
              " 'enough': tensor(0.0220),\n",
              " 'entered': tensor(-0.0533),\n",
              " 'entrance': tensor(0.1123),\n",
              " 'envelope': tensor(0.0623),\n",
              " 'er': tensor(-0.0809),\n",
              " 'erised': tensor(-0.0454),\n",
              " 'even': tensor(0.1317),\n",
              " 'evening': tensor(0.0055),\n",
              " 'ever': tensor(-0.0302),\n",
              " 'every': tensor(-0.0096),\n",
              " 'everybody': tensor(-0.0987),\n",
              " 'everyone': tensor(0.0630),\n",
              " 'everything': tensor(-0.1168),\n",
              " 'everywhere': tensor(0.0131),\n",
              " 'evil': tensor(-0.0536),\n",
              " 'exactly': tensor(-0.0879),\n",
              " 'exam': tensor(-0.1449),\n",
              " 'exams': tensor(0.0820),\n",
              " 'excellent': tensor(-0.0079),\n",
              " 'except': tensor(-0.0436),\n",
              " 'excitedly': tensor(-0.0025),\n",
              " 'excuse': tensor(-0.0656),\n",
              " 'expect': tensor(-0.0784),\n",
              " 'expected': tensor(-0.0271),\n",
              " 'expelled': tensor(-0.0277),\n",
              " 'explain': tensor(0.0437),\n",
              " 'extra': tensor(-0.0278),\n",
              " 'eye': tensor(0.1500),\n",
              " 'eyes': tensor(-0.1155),\n",
              " 'face': tensor(0.1663),\n",
              " 'faces': tensor(-0.0858),\n",
              " 'facing': tensor(0.0122),\n",
              " 'fact': tensor(0.0679),\n",
              " 'faded': tensor(0.0266),\n",
              " 'fall': tensor(-0.0131),\n",
              " 'fallen': tensor(-0.0427),\n",
              " 'families': tensor(0.0552),\n",
              " 'family': tensor(-0.1548),\n",
              " 'famous': tensor(-0.0386),\n",
              " 'fang': tensor(0.0841),\n",
              " 'fangs': tensor(0.2042),\n",
              " 'far': tensor(-0.1332),\n",
              " 'fast': tensor(0.0889),\n",
              " 'fat': tensor(-0.0583),\n",
              " 'father': tensor(-0.0586),\n",
              " 'fathers': tensor(-0.0188),\n",
              " 'favorite': tensor(-0.0105),\n",
              " 'fear': tensor(0.2072),\n",
              " 'feast': tensor(0.1502),\n",
              " 'feather': tensor(0.0423),\n",
              " 'feel': tensor(-0.2479),\n",
              " 'feeling': tensor(0.0714),\n",
              " 'feet': tensor(0.0149),\n",
              " 'fell': tensor(0.0277),\n",
              " 'felt': tensor(-0.1328),\n",
              " 'fer': tensor(0.0608),\n",
              " 'few': tensor(0.0069),\n",
              " 'field': tensor(-0.1686),\n",
              " 'fifty': tensor(-0.0982),\n",
              " 'fight': tensor(0.1233),\n",
              " 'fighting': tensor(-0.0458),\n",
              " 'figure': tensor(-0.0244),\n",
              " 'filch': tensor(-0.0613),\n",
              " 'filled': tensor(0.0470),\n",
              " 'finally': tensor(0.0570),\n",
              " 'find': tensor(-0.0859),\n",
              " 'finding': tensor(-0.0309),\n",
              " 'fine': tensor(0.1026),\n",
              " 'fingers': tensor(0.1057),\n",
              " 'finished': tensor(0.0967),\n",
              " 'finnigan': tensor(-0.1296),\n",
              " 'fire': tensor(0.1144),\n",
              " 'firenze': tensor(0.2032),\n",
              " 'firs': tensor(-0.0516),\n",
              " 'first': tensor(0.0264),\n",
              " 'five': tensor(-0.0190),\n",
              " 'fixed': tensor(0.0635),\n",
              " 'flamel': tensor(0.1211),\n",
              " 'flames': tensor(-0.0141),\n",
              " 'flash': tensor(-0.0056),\n",
              " 'flat': tensor(-0.1779),\n",
              " 'flavor': tensor(-0.2029),\n",
              " 'flew': tensor(0.1474),\n",
              " 'flint': tensor(0.0319),\n",
              " 'flitwick': tensor(0.2328),\n",
              " 'floating': tensor(0.0639),\n",
              " 'floor': tensor(0.0419),\n",
              " 'fluffy': tensor(-0.1233),\n",
              " 'flute': tensor(0.0844),\n",
              " 'fly': tensor(-0.0089),\n",
              " 'flying': tensor(-0.2095),\n",
              " 'follow': tensor(0.1519),\n",
              " 'followed': tensor(0.0465),\n",
              " 'following': tensor(-0.1083),\n",
              " 'food': tensor(-0.0180),\n",
              " 'foot': tensor(-0.0612),\n",
              " 'footsteps': tensor(-0.0704),\n",
              " 'for': tensor(0.1020),\n",
              " 'forbidden': tensor(0.1566),\n",
              " 'force': tensor(0.2266),\n",
              " 'forehead': tensor(0.1307),\n",
              " 'forest': tensor(0.0888),\n",
              " 'forget': tensor(0.1208),\n",
              " 'forgotten': tensor(-0.1553),\n",
              " 'forward': tensor(-0.1336),\n",
              " 'found': tensor(-0.1945),\n",
              " 'four': tensor(-0.0266),\n",
              " 'fred': tensor(0.0284),\n",
              " 'free': tensor(0.0832),\n",
              " 'friend': tensor(0.0423),\n",
              " 'friends': tensor(-0.1353),\n",
              " 'frog': tensor(0.1712),\n",
              " 'frogs': tensor(0.0902),\n",
              " 'from': tensor(-0.0475),\n",
              " 'front': tensor(0.0298),\n",
              " 'full': tensor(0.0236),\n",
              " 'fun': tensor(0.0936),\n",
              " 'funny': tensor(-0.2100),\n",
              " 'furious': tensor(-0.0638),\n",
              " 'furiously': tensor(-0.0702),\n",
              " 'game': tensor(-0.0485),\n",
              " 'garden': tensor(0.0251),\n",
              " 'gasped': tensor(0.0422),\n",
              " 'gave': tensor(0.0816),\n",
              " 'gently': tensor(-0.0237),\n",
              " 'george': tensor(-0.0079),\n",
              " 'get': tensor(-0.1606),\n",
              " 'gets': tensor(-0.0765),\n",
              " 'gettin': tensor(0.1643),\n",
              " 'getting': tensor(-0.1096),\n",
              " 'ghost': tensor(0.0982),\n",
              " 'ghosts': tensor(0.1040),\n",
              " 'giant': tensor(0.1256),\n",
              " 'girl': tensor(-0.1903),\n",
              " 'girls': tensor(-0.0295),\n",
              " 'give': tensor(-0.0506),\n",
              " 'given': tensor(-0.0287),\n",
              " 'giving': tensor(-0.0177),\n",
              " 'glad': tensor(0.0543),\n",
              " 'glass': tensor(0.0407),\n",
              " 'glasses': tensor(0.0566),\n",
              " 'go': tensor(-0.0602),\n",
              " 'goal': tensor(0.2412),\n",
              " 'goblin': tensor(0.0542),\n",
              " 'goblins': tensor(0.0252),\n",
              " 'goes': tensor(0.1358),\n",
              " 'going': tensor(0.0146),\n",
              " 'gold': tensor(-0.0224),\n",
              " 'golden': tensor(0.1150),\n",
              " 'gone': tensor(-0.0676),\n",
              " 'good': tensor(-0.1654),\n",
              " 'goodbye': tensor(0.1016),\n",
              " 'got': tensor(0.0306),\n",
              " 'gotta': tensor(-0.1002),\n",
              " 'gotten': tensor(0.0210),\n",
              " 'goyle': tensor(0.0925),\n",
              " 'grab': tensor(0.0616),\n",
              " 'grabbed': tensor(0.1084),\n",
              " 'granger': tensor(0.0588),\n",
              " 'grass': tensor(-0.0189),\n",
              " 'gray': tensor(-0.1210),\n",
              " 'great': tensor(0.0672),\n",
              " 'green': tensor(0.0209),\n",
              " 'grin': tensor(0.2537),\n",
              " 'gringotts': tensor(-0.1554),\n",
              " 'griphook': tensor(0.1449),\n",
              " 'ground': tensor(-0.0639),\n",
              " 'grounds': tensor(-0.0262),\n",
              " 'growled': tensor(0.0147),\n",
              " 'grunted': tensor(0.0259),\n",
              " 'gryffindor': tensor(0.0256),\n",
              " 'gryffindors': tensor(0.0259),\n",
              " 'guard': tensor(-0.3229),\n",
              " 'guarding': tensor(-0.1298),\n",
              " 'h': tensor(-0.0152),\n",
              " 'had': tensor(-0.0783),\n",
              " 'hadnt': tensor(-0.1421),\n",
              " 'hagrid': tensor(-0.0647),\n",
              " 'hagrids': tensor(-0.1920),\n",
              " 'hair': tensor(-0.1293),\n",
              " 'half': tensor(0.0408),\n",
              " 'halfway': tensor(0.1227),\n",
              " 'hall': tensor(0.0983),\n",
              " 'halloween': tensor(-0.0012),\n",
              " 'hand': tensor(-0.0737),\n",
              " 'handed': tensor(-0.0325),\n",
              " 'handle': tensor(-0.0077),\n",
              " 'hands': tensor(-0.0124),\n",
              " 'hang': tensor(0.0850),\n",
              " 'hanging': tensor(0.0358),\n",
              " 'happen': tensor(-0.0053),\n",
              " 'happened': tensor(0.1718),\n",
              " 'happy': tensor(-0.1334),\n",
              " 'hard': tensor(0.0572),\n",
              " 'harder': tensor(-0.1436),\n",
              " 'hardly': tensor(-0.0320),\n",
              " 'harry': tensor(1.0596),\n",
              " 'harrys': tensor(-0.0645),\n",
              " 'has': tensor(-0.0614),\n",
              " 'hasnt': tensor(0.0120),\n",
              " 'hat': tensor(0.1043),\n",
              " 'hate': tensor(-0.2282),\n",
              " 'hated': tensor(-0.0932),\n",
              " 'have': tensor(0.0338),\n",
              " 'havent': tensor(0.1875),\n",
              " 'having': tensor(0.1288),\n",
              " 'he': tensor(-0.0125),\n",
              " 'head': tensor(-0.2353),\n",
              " 'headless': tensor(0.0040),\n",
              " 'heads': tensor(0.0531),\n",
              " 'hear': tensor(0.0110),\n",
              " 'heard': tensor(-0.0889),\n",
              " 'heart': tensor(-0.1103),\n",
              " 'heavy': tensor(-0.0943),\n",
              " 'hed': tensor(0.0755),\n",
              " 'hedwig': tensor(-0.1833),\n",
              " 'held': tensor(-0.0616),\n",
              " 'hell': tensor(-0.0218),\n",
              " 'help': tensor(-0.0246),\n",
              " 'her': tensor(-0.1076),\n",
              " 'here': tensor(-0.0444),\n",
              " 'hermione': tensor(-0.0710),\n",
              " 'hermiones': tensor(0.0437),\n",
              " 'herself': tensor(0.0307),\n",
              " 'hes': tensor(0.0251),\n",
              " 'hidden': tensor(0.1589),\n",
              " 'hide': tensor(0.0622),\n",
              " 'hiding': tensor(-0.1446),\n",
              " 'high': tensor(-0.1002),\n",
              " 'higher': tensor(-0.0638),\n",
              " 'him': tensor(0.0814),\n",
              " 'himself': tensor(0.0623),\n",
              " 'his': tensor(0.2304),\n",
              " 'hissed': tensor(0.0472),\n",
              " 'history': tensor(-0.0906),\n",
              " 'hit': tensor(0.0676),\n",
              " 'hogwarts': tensor(0.0142),\n",
              " 'hold': tensor(0.0719),\n",
              " 'holding': tensor(-0.0029),\n",
              " 'hole': tensor(0.1191),\n",
              " 'holidays': tensor(-0.0683),\n",
              " 'home': tensor(0.1089),\n",
              " 'homework': tensor(-0.1210),\n",
              " 'honestly': tensor(0.1526),\n",
              " 'hooch': tensor(-0.0417),\n",
              " 'hoops': tensor(0.0916),\n",
              " 'hope': tensor(0.0842),\n",
              " 'hoping': tensor(0.0057),\n",
              " 'horrible': tensor(0.0594),\n",
              " 'horror': tensor(0.1765),\n",
              " 'hospital': tensor(-0.0050),\n",
              " 'hot': tensor(-0.1463),\n",
              " 'hour': tensor(0.0737),\n",
              " 'hours': tensor(-0.0630),\n",
              " 'house': tensor(0.0705),\n",
              " 'houses': tensor(-0.0904),\n",
              " 'how': tensor(-0.1960),\n",
              " 'however': tensor(-0.0219),\n",
              " 'howling': tensor(0.0467),\n",
              " 'hufflepuff': tensor(0.1178),\n",
              " 'huge': tensor(0.0984),\n",
              " 'human': tensor(0.0441),\n",
              " 'hundred': tensor(0.1886),\n",
              " 'hundreds': tensor(-0.0309),\n",
              " 'hung': tensor(-0.0916),\n",
              " 'hungry': tensor(0.0939),\n",
              " 'hurried': tensor(0.1220),\n",
              " 'hurry': tensor(-0.1811),\n",
              " 'hurrying': tensor(-0.2300),\n",
              " 'hurt': tensor(-0.1147),\n",
              " 'hut': tensor(0.1670),\n",
              " 'i': tensor(0.0318),\n",
              " 'ice': tensor(-0.0213),\n",
              " 'id': tensor(-0.0330),\n",
              " 'idea': tensor(-0.0324),\n",
              " 'if': tensor(-0.0282),\n",
              " 'ignored': tensor(-0.0908),\n",
              " 'ill': tensor(-0.0924),\n",
              " 'im': tensor(0.1163),\n",
              " 'imagine': tensor(0.1298),\n",
              " 'important': tensor(-0.1145),\n",
              " 'in': tensor(-0.0874),\n",
              " 'inches': tensor(0.1224),\n",
              " 'indeed': tensor(0.0181),\n",
              " 'inside': tensor(-0.0010),\n",
              " 'instead': tensor(0.0329),\n",
              " 'interested': tensor(0.0509),\n",
              " 'interesting': tensor(-0.0222),\n",
              " 'into': tensor(-0.1003),\n",
              " 'invisibility': tensor(-0.0623),\n",
              " 'invisible': tensor(-0.0740),\n",
              " 'is': tensor(-0.1804),\n",
              " 'isnt': tensor(-0.1389),\n",
              " 'it': tensor(0.0961),\n",
              " 'itll': tensor(-0.1283),\n",
              " 'its': tensor(0.1201),\n",
              " 'itself': tensor(-0.0459),\n",
              " 'ive': tensor(0.0749),\n",
              " 'jerked': tensor(-0.0293),\n",
              " 'job': tensor(0.1291),\n",
              " 'join': tensor(0.0636),\n",
              " 'joined': tensor(-0.1473),\n",
              " 'joke': tensor(-0.1099),\n",
              " 'jordan': tensor(-0.0697),\n",
              " 'jump': tensor(0.2078),\n",
              " 'jumped': tensor(0.0299),\n",
              " 'jus': tensor(0.0012),\n",
              " 'just': tensor(0.0954),\n",
              " 'keep': tensor(-0.0949),\n",
              " 'keeper': tensor(0.0734),\n",
              " 'keeping': tensor(-0.1524),\n",
              " 'kept': tensor(-0.0580),\n",
              " 'key': tensor(0.1188),\n",
              " 'keys': tensor(-0.0306),\n",
              " 'kicked': tensor(0.0750),\n",
              " 'kill': tensor(0.0069),\n",
              " 'killed': tensor(0.0033),\n",
              " 'kind': tensor(0.0314),\n",
              " 'kings': tensor(-0.0291),\n",
              " 'kitchen': tensor(-0.0245),\n",
              " 'knees': tensor(0.1618),\n",
              " 'knew': tensor(-0.0344),\n",
              " 'knight': tensor(-0.0346),\n",
              " 'knock': tensor(-0.0424),\n",
              " 'knocked': tensor(-0.0844),\n",
              " 'knocking': tensor(-0.0330),\n",
              " 'know': tensor(0.0804),\n",
              " 'knowing': tensor(0.0811),\n",
              " 'known': tensor(-0.0211),\n",
              " 'knows': tensor(-0.0040),\n",
              " 'knuts': tensor(0.2100),\n",
              " 'lady': tensor(-0.0111),\n",
              " 'lake': tensor(-0.1161),\n",
              " 'lamp': tensor(0.0918),\n",
              " 'landed': tensor(0.1013),\n",
              " 'large': tensor(-0.1319),\n",
              " 'last': tensor(0.1479),\n",
              " 'late': tensor(-0.0127),\n",
              " 'later': tensor(-0.0610),\n",
              " 'laugh': tensor(0.0287),\n",
              " 'laughed': tensor(0.0121),\n",
              " 'laughing': tensor(0.1471),\n",
              " 'laughter': tensor(0.1536),\n",
              " 'lay': tensor(-0.0827),\n",
              " 'lead': tensor(0.1325),\n",
              " 'leading': tensor(-0.0233),\n",
              " 'leaky': tensor(-0.0751),\n",
              " 'leaned': tensor(0.0494),\n",
              " 'leapt': tensor(-0.0051),\n",
              " 'learn': tensor(-0.0916),\n",
              " 'learned': tensor(-0.1847),\n",
              " 'least': tensor(-0.0700),\n",
              " 'leave': tensor(0.0704),\n",
              " 'leaves': tensor(-0.0189),\n",
              " 'leaving': tensor(-0.0022),\n",
              " 'led': tensor(-0.0172),\n",
              " 'lee': tensor(0.0837),\n",
              " 'left': tensor(-0.1443),\n",
              " 'leg': tensor(-0.0829),\n",
              " 'legs': tensor(-0.0312),\n",
              " 'lemon': tensor(0.0563),\n",
              " 'less': tensor(0.1083),\n",
              " 'lesson': tensor(0.0062),\n",
              " 'lessons': tensor(0.1714),\n",
              " 'let': tensor(0.0102),\n",
              " 'lets': tensor(-0.1207),\n",
              " 'letter': tensor(-0.1230),\n",
              " 'letters': tensor(0.0121),\n",
              " 'library': tensor(0.0040),\n",
              " 'lie': tensor(-0.1198),\n",
              " 'life': tensor(-0.1411),\n",
              " 'light': tensor(0.0690),\n",
              " 'lightning': tensor(-0.1381),\n",
              " 'like': tensor(0.1449),\n",
              " 'liked': tensor(0.1556),\n",
              " 'lily': tensor(-0.0107),\n",
              " 'line': tensor(0.0774),\n",
              " 'lips': tensor(0.1108),\n",
              " 'list': tensor(0.0198),\n",
              " 'listen': tensor(0.0454),\n",
              " 'listening': tensor(0.1575),\n",
              " 'lit': tensor(-0.1236),\n",
              " 'little': tensor(0.1612),\n",
              " 'live': tensor(-0.1001),\n",
              " 'lived': tensor(0.0139),\n",
              " 'living': tensor(0.0757),\n",
              " 'loads': tensor(-0.0695),\n",
              " 'lock': tensor(0.1904),\n",
              " 'locked': tensor(0.0050),\n",
              " 'london': tensor(0.1414),\n",
              " 'long': tensor(0.0798),\n",
              " 'longbottom': tensor(0.0501),\n",
              " 'look': tensor(-0.1327),\n",
              " 'looked': tensor(0.0123),\n",
              " 'looking': tensor(-0.0341),\n",
              " 'looks': tensor(-0.1675),\n",
              " 'lose': tensor(0.0480),\n",
              " 'losing': tensor(0.1094),\n",
              " 'lost': tensor(0.1653),\n",
              " 'lot': tensor(0.0848),\n",
              " 'lots': tensor(-0.0305),\n",
              " 'loud': tensor(0.0143),\n",
              " 'loudly': tensor(-0.0028),\n",
              " 'low': tensor(0.1141),\n",
              " 'luck': tensor(-0.1315),\n",
              " 'lucky': tensor(0.2334),\n",
              " 'lumpy': tensor(0.0090),\n",
              " 'lurking': tensor(0.0434),\n",
              " 'lying': tensor(0.2726),\n",
              " 'mad': tensor(0.0941),\n",
              " 'madam': tensor(0.0150),\n",
              " 'made': tensor(0.1348),\n",
              " 'magic': tensor(-0.0466),\n",
              " 'magical': tensor(0.0262),\n",
              " 'mail': tensor(0.0730),\n",
              " 'make': tensor(0.0528),\n",
              " 'making': tensor(-0.0077),\n",
              " 'malfoy': tensor(-0.0543),\n",
              " 'malfoys': tensor(-0.0303),\n",
              " 'man': tensor(0.1303),\n",
              " 'managed': tensor(-0.0868),\n",
              " 'many': tensor(-0.0099),\n",
              " 'marble': tensor(-0.1361),\n",
              " 'marched': tensor(0.0591),\n",
              " 'match': tensor(0.0788),\n",
              " 'matter': tensor(0.1816),\n",
              " 'may': tensor(0.1709),\n",
              " 'maybe': tensor(-0.1014),\n",
              " 'mcgonagall': tensor(0.0006),\n",
              " 'mcgonagalls': tensor(-0.0445),\n",
              " 'me': tensor(-0.0071),\n",
              " 'mean': tensor(-0.0549),\n",
              " 'means': tensor(0.0466),\n",
              " 'meant': tensor(-0.1495),\n",
              " 'meet': tensor(0.0150),\n",
              " 'mention': tensor(-0.1764),\n",
              " 'met': tensor(0.0617),\n",
              " 'midair': tensor(0.0323),\n",
              " 'middle': tensor(-0.0562),\n",
              " 'midnight': tensor(0.1326),\n",
              " 'might': tensor(-0.0959),\n",
              " 'mind': tensor(-0.2559),\n",
              " 'ministry': tensor(0.1059),\n",
              " 'minute': tensor(-0.1126),\n",
              " 'minutes': tensor(-0.0268),\n",
              " 'mirror': tensor(-0.1872),\n",
              " 'miss': tensor(-0.1041),\n",
              " 'mistake': tensor(0.0208),\n",
              " 'moaned': tensor(-0.0629),\n",
              " 'mom': tensor(0.0412),\n",
              " 'moment': tensor(-0.2392),\n",
              " 'money': tensor(0.0816),\n",
              " 'moonlight': tensor(-0.0117),\n",
              " 'more': tensor(-0.0234),\n",
              " 'morning': tensor(0.1056),\n",
              " 'most': tensor(0.1126),\n",
              " 'mother': tensor(-0.0069),\n",
              " 'mothers': tensor(-0.0581),\n",
              " 'motorcycle': tensor(-0.0757),\n",
              " 'mountain': tensor(0.1054),\n",
              " 'mouth': tensor(0.0266),\n",
              " 'move': tensor(-0.0017),\n",
              " 'moved': tensor(-0.1199),\n",
              " 'moving': tensor(-0.0812),\n",
              " 'mr': tensor(-0.0302),\n",
              " 'mrs': tensor(0.0404),\n",
              " 'much': tensor(-0.0579),\n",
              " 'muggle': tensor(0.1613),\n",
              " 'muggles': tensor(0.1390),\n",
              " 'murmured': tensor(0.0840),\n",
              " 'must': tensor(-0.0969),\n",
              " 'mustache': tensor(0.0887),\n",
              " 'mustve': tensor(0.0826),\n",
              " 'muttered': tensor(0.0048),\n",
              " 'muttering': tensor(0.1476),\n",
              " 'my': tensor(-0.2203),\n",
              " 'myself': tensor(-0.1352),\n",
              " 'mysterious': tensor(0.0847),\n",
              " 'nah': tensor(0.0965),\n",
              " 'name': tensor(0.2181),\n",
              " 'names': tensor(-0.0205),\n",
              " 'narrow': tensor(0.0060),\n",
              " 'nasty': tensor(0.1362),\n",
              " 'near': tensor(0.0057),\n",
              " 'nearer': tensor(0.0699),\n",
              " 'nearest': tensor(-0.0331),\n",
              " 'nearly': tensor(-0.1618),\n",
              " 'neck': tensor(-0.0598),\n",
              " 'need': tensor(-0.1172),\n",
              " 'needed': tensor(0.2126),\n",
              " 'needs': tensor(-0.0362),\n",
              " 'neither': tensor(-0.1673),\n",
              " 'nervous': tensor(-0.0569),\n",
              " 'nervously': tensor(-0.2198),\n",
              " 'never': tensor(-0.0109),\n",
              " 'neville': tensor(-0.0505),\n",
              " 'nevilles': tensor(0.1329),\n",
              " 'new': tensor(-0.0473),\n",
              " 'news': tensor(-0.1015),\n",
              " 'newspaper': tensor(-0.0247),\n",
              " 'next': tensor(0.1195),\n",
              " 'nice': tensor(-0.0728),\n",
              " 'nicolas': tensor(0.0135),\n",
              " 'night': tensor(-0.0938),\n",
              " 'nimbus': tensor(0.1317),\n",
              " 'nine': tensor(0.0134),\n",
              " 'no': tensor(0.0917),\n",
              " 'nobody': tensor(0.1332),\n",
              " 'nodded': tensor(-0.0745),\n",
              " 'noise': tensor(-0.1991),\n",
              " 'none': tensor(0.0201),\n",
              " 'nor': tensor(-0.0850),\n",
              " 'norbert': tensor(0.0538),\n",
              " 'normal': tensor(-0.0351),\n",
              " 'norris': tensor(0.1354),\n",
              " 'nose': tensor(0.0616),\n",
              " 'noses': tensor(0.0679),\n",
              " 'nostrils': tensor(0.0297),\n",
              " 'not': tensor(0.1303),\n",
              " 'note': tensor(-0.0910),\n",
              " 'notes': tensor(-0.0084),\n",
              " 'nothin': tensor(0.0249),\n",
              " 'nothing': tensor(0.0868),\n",
              " 'notice': tensor(-0.0520),\n",
              " 'noticed': tensor(-0.1033),\n",
              " 'noticing': tensor(0.0347),\n",
              " 'now': tensor(-0.0150),\n",
              " 'number': tensor(-0.1220),\n",
              " 'o': tensor(-0.0113),\n",
              " 'obviously': tensor(0.0744),\n",
              " 'oclock': tensor(0.0749),\n",
              " 'odd': tensor(0.0355),\n",
              " 'of': tensor(0.0315),\n",
              " 'off': tensor(-0.0881),\n",
              " 'often': tensor(-0.0387),\n",
              " 'oh': tensor(-0.0140),\n",
              " 'old': tensor(0.0187),\n",
              " 'older': tensor(-0.0984),\n",
              " 'ollivander': tensor(-0.0666),\n",
              " 'on': tensor(0.0065),\n",
              " 'once': tensor(-0.0050),\n",
              " 'one': tensor(-0.1521),\n",
              " 'ones': tensor(0.0328),\n",
              " 'only': tensor(-0.2657),\n",
              " 'onto': tensor(-0.0116),\n",
              " 'open': tensor(-0.2938),\n",
              " 'opened': tensor(-0.0706),\n",
              " 'opposite': tensor(0.1047),\n",
              " 'or': tensor(0.2524),\n",
              " 'ordinary': tensor(-0.0136),\n",
              " 'other': tensor(0.0158),\n",
              " 'others': tensor(-0.0287),\n",
              " 'our': tensor(0.0394),\n",
              " 'out': tensor(-0.0395),\n",
              " 'outside': tensor(0.1027),\n",
              " 'outta': tensor(-0.1488),\n",
              " 'over': tensor(0.0384),\n",
              " 'overhead': tensor(-0.0971),\n",
              " 'owl': tensor(-0.0513),\n",
              " 'owls': tensor(-0.0629),\n",
              " 'own': tensor(-0.1830),\n",
              " 'pack': tensor(0.0230),\n",
              " 'package': tensor(-0.1034),\n",
              " 'packed': tensor(0.0196),\n",
              " 'pain': tensor(-0.0620),\n",
              " 'pair': tensor(0.1661),\n",
              " 'pale': tensor(0.0979),\n",
              " 'panted': tensor(0.0273),\n",
              " 'paper': tensor(-0.1383),\n",
              " 'parcel': tensor(0.1422),\n",
              " 'parchment': tensor(0.0468),\n",
              " 'parents': tensor(0.1324),\n",
              " 'particularly': tensor(0.0033),\n",
              " 'passageway': tensor(0.0333),\n",
              " 'passed': tensor(-0.0478),\n",
              " 'passing': tensor(-0.0780),\n",
              " 'past': tensor(0.0447),\n",
              " 'path': tensor(0.1603),\n",
              " 'patil': tensor(-0.0462),\n",
              " 'peered': tensor(-0.1030),\n",
              " 'peering': tensor(-0.0111),\n",
              " 'peeves': tensor(0.0416),\n",
              " 'people': tensor(-0.0275),\n",
              " 'percy': tensor(-0.1154),\n",
              " 'perfect': tensor(-0.0177),\n",
              " 'perhaps': tensor(0.1667),\n",
              " 'person': tensor(0.0496),\n",
              " 'petunia': tensor(-0.0599),\n",
              " 'picked': tensor(0.0931),\n",
              " 'piece': tensor(0.2029),\n",
              " 'pieces': tensor(0.0528),\n",
              " 'piers': tensor(-0.0978),\n",
              " 'pig': tensor(-0.0113),\n",
              " 'pile': tensor(0.0743),\n",
              " 'piled': tensor(0.0235),\n",
              " 'pink': tensor(0.0778),\n",
              " 'pinned': tensor(0.0596),\n",
              " 'place': tensor(0.1499),\n",
              " 'planets': tensor(0.1872),\n",
              " 'plant': tensor(0.1222),\n",
              " 'plates': tensor(-0.1680),\n",
              " 'platform': tensor(0.0827),\n",
              " 'platforms': tensor(-0.0026),\n",
              " 'play': tensor(-0.0338),\n",
              " 'players': tensor(-0.2017),\n",
              " 'playing': tensor(-0.0662),\n",
              " 'please': tensor(0.0899),\n",
              " 'pleased': tensor(-0.0321),\n",
              " 'pocket': tensor(-0.0199),\n",
              " 'pockets': tensor(0.0217),\n",
              " 'point': tensor(-0.0055),\n",
              " 'pointed': tensor(-0.0550),\n",
              " 'pointing': tensor(0.0239),\n",
              " 'points': tensor(-0.0273),\n",
              " 'pomfrey': tensor(-0.0501),\n",
              " 'poor': tensor(-0.0174),\n",
              " 'portrait': tensor(0.1800),\n",
              " 'possession': tensor(-0.0136),\n",
              " 'possible': tensor(0.0447),\n",
              " 'posts': tensor(0.0460),\n",
              " 'potion': tensor(0.0411),\n",
              " 'potions': tensor(-0.0681),\n",
              " 'potter': tensor(0.0024),\n",
              " 'potters': tensor(0.1015),\n",
              " 'power': tensor(-0.1127),\n",
              " 'powerful': tensor(0.1402),\n",
              " 'practice': tensor(0.0724),\n",
              " 'prefect': tensor(-0.0801),\n",
              " 'prefects': tensor(-0.0201),\n",
              " 'presents': tensor(0.0617),\n",
              " 'pressed': tensor(0.2501),\n",
              " 'privet': tensor(0.1368),\n",
              " 'probably': tensor(0.2313),\n",
              " 'professor': tensor(0.0909),\n",
              " 'properly': tensor(-0.1065),\n",
              " 'proud': tensor(-0.0961),\n",
              " 'pull': tensor(0.0098),\n",
              " 'pulled': tensor(-0.0721),\n",
              " 'pulling': tensor(-0.0227),\n",
              " 'purple': tensor(-0.0662),\n",
              " 'pushed': tensor(-0.0854),\n",
              " 'put': tensor(-0.0957),\n",
              " 'quaffle': tensor(-0.0097),\n",
              " 'question': tensor(-0.0446),\n",
              " 'questions': tensor(0.0122),\n",
              " 'quick': tensor(-0.1756),\n",
              " 'quickly': tensor(-0.0976),\n",
              " 'quidditch': tensor(-0.1884),\n",
              " 'quiet': tensor(-0.0963),\n",
              " 'quietly': tensor(-0.0494),\n",
              " 'quills': tensor(0.0301),\n",
              " 'quirrell': tensor(0.1302),\n",
              " 'quirrells': tensor(-0.0060),\n",
              " 'quite': tensor(-0.1325),\n",
              " 'racing': tensor(-0.0971),\n",
              " 'raised': tensor(-0.1635),\n",
              " 'ran': tensor(-0.0128),\n",
              " 'rang': tensor(-0.0449),\n",
              " 'rat': tensor(0.1776),\n",
              " 'rather': tensor(-0.0282),\n",
              " 'ravenclaw': tensor(0.0605),\n",
              " 'reached': tensor(-0.0666),\n",
              " 'read': tensor(-0.1498),\n",
              " 'ready': tensor(-0.0551),\n",
              " 'real': tensor(-0.1030),\n",
              " 'realize': tensor(0.1832),\n",
              " 'realized': tensor(0.0059),\n",
              " 'really': tensor(0.0584),\n",
              " 'reason': tensor(-0.1070),\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# we can get the dot product value for every other words in the vocab\n",
        "# to get  P(word | harry)\n",
        "word_dot_dict = {}\n",
        "for word in filtered_vocab:\n",
        "  w_idx = word2idx[word]\n",
        "  w_vector = word_vectors[w_idx]\n",
        "  word_dot_dict[word] = sum(w_vector * harry)\n",
        "word_dot_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXrRV6xoyhDY"
      },
      "source": [
        "Now, let's convert these dot products to probabilities using the softmax function:\n",
        "- We have to convert our prediction into probability distribution to get P(word|harry) so that sum of [P(a|harry), ..., P(potter|harry), ... P(ron|harry), ... ] = 1\n",
        "- current dot product value is any real number, sometimes called as logit\n",
        "  - logit from logistic regression. Some values that are not yet converted to 0-1 or value before sigmoid function\n",
        "  - every probability should be in range (0, 1) (greater than 0, smaller than 1)\n",
        "  - this can be handled by taking exponential of dot product values, divided by total sum\n",
        "  - This function is called **Softmax**\n",
        "\n",
        "- Why we use exponential?\n",
        "  - Because we want to make every probability in positive range while preserving the order\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "DQ1PUvuLyv6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d9c3bfb3-f5e4-4f96-a74b-a3e618b541ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': tensor(0.0007),\n",
              " 'able': tensor(0.0007),\n",
              " 'abou': tensor(0.0006),\n",
              " 'about': tensor(0.0008),\n",
              " 'above': tensor(0.0006),\n",
              " 'across': tensor(0.0005),\n",
              " 'added': tensor(0.0006),\n",
              " 'afford': tensor(0.0007),\n",
              " 'afraid': tensor(0.0006),\n",
              " 'after': tensor(0.0007),\n",
              " 'afternoon': tensor(0.0005),\n",
              " 'again': tensor(0.0008),\n",
              " 'against': tensor(0.0006),\n",
              " 'ages': tensor(0.0006),\n",
              " 'ago': tensor(0.0006),\n",
              " 'agreed': tensor(0.0007),\n",
              " 'ah': tensor(0.0008),\n",
              " 'ahead': tensor(0.0006),\n",
              " 'air': tensor(0.0007),\n",
              " 'albus': tensor(0.0008),\n",
              " 'alive': tensor(0.0007),\n",
              " 'all': tensor(0.0006),\n",
              " 'alley': tensor(0.0006),\n",
              " 'allowed': tensor(0.0006),\n",
              " 'almost': tensor(0.0006),\n",
              " 'alone': tensor(0.0007),\n",
              " 'along': tensor(0.0007),\n",
              " 'already': tensor(0.0005),\n",
              " 'also': tensor(0.0007),\n",
              " 'although': tensor(0.0006),\n",
              " 'always': tensor(0.0006),\n",
              " 'am': tensor(0.0008),\n",
              " 'an': tensor(0.0006),\n",
              " 'and': tensor(0.0006),\n",
              " 'angrily': tensor(0.0006),\n",
              " 'angry': tensor(0.0007),\n",
              " 'another': tensor(0.0007),\n",
              " 'answer': tensor(0.0007),\n",
              " 'any': tensor(0.0007),\n",
              " 'anymore': tensor(0.0006),\n",
              " 'anyone': tensor(0.0007),\n",
              " 'anythin': tensor(0.0006),\n",
              " 'anything': tensor(0.0007),\n",
              " 'anyway': tensor(0.0006),\n",
              " 'anywhere': tensor(0.0006),\n",
              " 'apart': tensor(0.0005),\n",
              " 'appeared': tensor(0.0007),\n",
              " 'are': tensor(0.0007),\n",
              " 'arent': tensor(0.0007),\n",
              " 'arm': tensor(0.0007),\n",
              " 'armor': tensor(0.0006),\n",
              " 'arms': tensor(0.0006),\n",
              " 'around': tensor(0.0006),\n",
              " 'arrived': tensor(0.0006),\n",
              " 'arts': tensor(0.0005),\n",
              " 'as': tensor(0.0006),\n",
              " 'ask': tensor(0.0006),\n",
              " 'asked': tensor(0.0006),\n",
              " 'asking': tensor(0.0007),\n",
              " 'asleep': tensor(0.0006),\n",
              " 'at': tensor(0.0007),\n",
              " 'attention': tensor(0.0006),\n",
              " 'aunt': tensor(0.0006),\n",
              " 'awake': tensor(0.0007),\n",
              " 'away': tensor(0.0006),\n",
              " 'baby': tensor(0.0006),\n",
              " 'back': tensor(0.0008),\n",
              " 'backward': tensor(0.0007),\n",
              " 'bacon': tensor(0.0006),\n",
              " 'bad': tensor(0.0007),\n",
              " 'bag': tensor(0.0006),\n",
              " 'ball': tensor(0.0007),\n",
              " 'balls': tensor(0.0006),\n",
              " 'bane': tensor(0.0007),\n",
              " 'barrier': tensor(0.0006),\n",
              " 'be': tensor(0.0007),\n",
              " 'beans': tensor(0.0007),\n",
              " 'beard': tensor(0.0006),\n",
              " 'became': tensor(0.0007),\n",
              " 'because': tensor(0.0007),\n",
              " 'become': tensor(0.0007),\n",
              " 'bed': tensor(0.0007),\n",
              " 'bedroom': tensor(0.0007),\n",
              " 'been': tensor(0.0006),\n",
              " 'before': tensor(0.0006),\n",
              " 'began': tensor(0.0006),\n",
              " 'behind': tensor(0.0007),\n",
              " 'being': tensor(0.0007),\n",
              " 'believe': tensor(0.0006),\n",
              " 'below': tensor(0.0006),\n",
              " 'beneath': tensor(0.0007),\n",
              " 'bent': tensor(0.0008),\n",
              " 'best': tensor(0.0006),\n",
              " 'bet': tensor(0.0008),\n",
              " 'better': tensor(0.0006),\n",
              " 'between': tensor(0.0007),\n",
              " 'big': tensor(0.0008),\n",
              " 'bill': tensor(0.0007),\n",
              " 'bin': tensor(0.0007),\n",
              " 'binoculars': tensor(0.0007),\n",
              " 'birthday': tensor(0.0006),\n",
              " 'bit': tensor(0.0006),\n",
              " 'black': tensor(0.0007),\n",
              " 'blankets': tensor(0.0006),\n",
              " 'blew': tensor(0.0006),\n",
              " 'blood': tensor(0.0006),\n",
              " 'bloody': tensor(0.0007),\n",
              " 'bludger': tensor(0.0006),\n",
              " 'bludgers': tensor(0.0007),\n",
              " 'blue': tensor(0.0008),\n",
              " 'board': tensor(0.0008),\n",
              " 'boat': tensor(0.0006),\n",
              " 'boats': tensor(0.0007),\n",
              " 'body': tensor(0.0007),\n",
              " 'book': tensor(0.0008),\n",
              " 'books': tensor(0.0006),\n",
              " 'both': tensor(0.0006),\n",
              " 'bottle': tensor(0.0007),\n",
              " 'bottles': tensor(0.0006),\n",
              " 'bottom': tensor(0.0005),\n",
              " 'bought': tensor(0.0007),\n",
              " 'bowed': tensor(0.0007),\n",
              " 'box': tensor(0.0006),\n",
              " 'boy': tensor(0.0006),\n",
              " 'boys': tensor(0.0006),\n",
              " 'branches': tensor(0.0007),\n",
              " 'brave': tensor(0.0007),\n",
              " 'break': tensor(0.0006),\n",
              " 'breakfast': tensor(0.0007),\n",
              " 'breaking': tensor(0.0006),\n",
              " 'breath': tensor(0.0007),\n",
              " 'breathing': tensor(0.0007),\n",
              " 'bright': tensor(0.0006),\n",
              " 'brilliant': tensor(0.0008),\n",
              " 'broke': tensor(0.0007),\n",
              " 'broken': tensor(0.0006),\n",
              " 'broom': tensor(0.0006),\n",
              " 'brooms': tensor(0.0006),\n",
              " 'broomstick': tensor(0.0006),\n",
              " 'broomsticks': tensor(0.0006),\n",
              " 'brother': tensor(0.0008),\n",
              " 'brothers': tensor(0.0007),\n",
              " 'brought': tensor(0.0007),\n",
              " 'brown': tensor(0.0006),\n",
              " 'burst': tensor(0.0006),\n",
              " 'business': tensor(0.0007),\n",
              " 'busy': tensor(0.0006),\n",
              " 'but': tensor(0.0007),\n",
              " 'buy': tensor(0.0006),\n",
              " 'by': tensor(0.0008),\n",
              " 'cake': tensor(0.0007),\n",
              " 'cakes': tensor(0.0006),\n",
              " 'call': tensor(0.0007),\n",
              " 'called': tensor(0.0007),\n",
              " 'came': tensor(0.0007),\n",
              " 'can': tensor(0.0007),\n",
              " 'cant': tensor(0.0006),\n",
              " 'car': tensor(0.0006),\n",
              " 'card': tensor(0.0006),\n",
              " 'care': tensor(0.0006),\n",
              " 'careful': tensor(0.0007),\n",
              " 'carefully': tensor(0.0007),\n",
              " 'carried': tensor(0.0006),\n",
              " 'carrying': tensor(0.0007),\n",
              " 'cart': tensor(0.0007),\n",
              " 'case': tensor(0.0006),\n",
              " 'castle': tensor(0.0006),\n",
              " 'cat': tensor(0.0007),\n",
              " 'catch': tensor(0.0007),\n",
              " 'cats': tensor(0.0006),\n",
              " 'caught': tensor(0.0006),\n",
              " 'cauldron': tensor(0.0007),\n",
              " 'cause': tensor(0.0006),\n",
              " 'ceiling': tensor(0.0008),\n",
              " 'centaur': tensor(0.0006),\n",
              " 'certainly': tensor(0.0007),\n",
              " 'chair': tensor(0.0007),\n",
              " 'chamber': tensor(0.0007),\n",
              " 'chance': tensor(0.0007),\n",
              " 'change': tensor(0.0007),\n",
              " 'changed': tensor(0.0007),\n",
              " 'chapter': tensor(0.0007),\n",
              " 'charlie': tensor(0.0006),\n",
              " 'charlies': tensor(0.0006),\n",
              " 'charms': tensor(0.0007),\n",
              " 'chasers': tensor(0.0006),\n",
              " 'cheer': tensor(0.0006),\n",
              " 'cheering': tensor(0.0007),\n",
              " 'cheers': tensor(0.0006),\n",
              " 'chess': tensor(0.0006),\n",
              " 'chessmen': tensor(0.0007),\n",
              " 'chest': tensor(0.0006),\n",
              " 'chocolate': tensor(0.0007),\n",
              " 'christmas': tensor(0.0006),\n",
              " 'chuckled': tensor(0.0007),\n",
              " 'clambered': tensor(0.0007),\n",
              " 'clapped': tensor(0.0007),\n",
              " 'class': tensor(0.0006),\n",
              " 'classes': tensor(0.0007),\n",
              " 'classroom': tensor(0.0006),\n",
              " 'clean': tensor(0.0006),\n",
              " 'clear': tensor(0.0005),\n",
              " 'cleared': tensor(0.0006),\n",
              " 'clearing': tensor(0.0007),\n",
              " 'clearly': tensor(0.0007),\n",
              " 'clicked': tensor(0.0006),\n",
              " 'climbed': tensor(0.0007),\n",
              " 'cloak': tensor(0.0007),\n",
              " 'close': tensor(0.0006),\n",
              " 'closer': tensor(0.0006),\n",
              " 'clothes': tensor(0.0007),\n",
              " 'club': tensor(0.0008),\n",
              " 'clutching': tensor(0.0007),\n",
              " 'coat': tensor(0.0007),\n",
              " 'cold': tensor(0.0006),\n",
              " 'come': tensor(0.0006),\n",
              " 'coming': tensor(0.0007),\n",
              " 'common': tensor(0.0007),\n",
              " 'compartment': tensor(0.0007),\n",
              " 'completely': tensor(0.0007),\n",
              " 'computer': tensor(0.0006),\n",
              " 'control': tensor(0.0006),\n",
              " 'corner': tensor(0.0007),\n",
              " 'corridor': tensor(0.0007),\n",
              " 'corridors': tensor(0.0006),\n",
              " 'could': tensor(0.0007),\n",
              " 'couldnt': tensor(0.0007),\n",
              " 'couple': tensor(0.0006),\n",
              " 'courage': tensor(0.0007),\n",
              " 'course': tensor(0.0006),\n",
              " 'covered': tensor(0.0006),\n",
              " 'crabbe': tensor(0.0005),\n",
              " 'crack': tensor(0.0007),\n",
              " 'crash': tensor(0.0007),\n",
              " 'crate': tensor(0.0006),\n",
              " 'crept': tensor(0.0007),\n",
              " 'cried': tensor(0.0007),\n",
              " 'cross': tensor(0.0007),\n",
              " 'crossed': tensor(0.0007),\n",
              " 'crowd': tensor(0.0008),\n",
              " 'cry': tensor(0.0008),\n",
              " 'crying': tensor(0.0006),\n",
              " 'cup': tensor(0.0007),\n",
              " 'cupboard': tensor(0.0007),\n",
              " 'curious': tensor(0.0007),\n",
              " 'curse': tensor(0.0007),\n",
              " 'cut': tensor(0.0007),\n",
              " 'dad': tensor(0.0007),\n",
              " 'damp': tensor(0.0007),\n",
              " 'dangerous': tensor(0.0007),\n",
              " 'dare': tensor(0.0006),\n",
              " 'dark': tensor(0.0007),\n",
              " 'darkly': tensor(0.0008),\n",
              " 'darkness': tensor(0.0006),\n",
              " 'day': tensor(0.0007),\n",
              " 'days': tensor(0.0007),\n",
              " 'dead': tensor(0.0006),\n",
              " 'dean': tensor(0.0006),\n",
              " 'dear': tensor(0.0007),\n",
              " 'death': tensor(0.0006),\n",
              " 'decided': tensor(0.0006),\n",
              " 'deep': tensor(0.0008),\n",
              " 'delighted': tensor(0.0007),\n",
              " 'desk': tensor(0.0007),\n",
              " 'desperate': tensor(0.0007),\n",
              " 'diagon': tensor(0.0009),\n",
              " 'did': tensor(0.0007),\n",
              " 'didnt': tensor(0.0006),\n",
              " 'die': tensor(0.0007),\n",
              " 'died': tensor(0.0007),\n",
              " 'difference': tensor(0.0006),\n",
              " 'different': tensor(0.0007),\n",
              " 'difficult': tensor(0.0007),\n",
              " 'dinner': tensor(0.0007),\n",
              " 'direction': tensor(0.0006),\n",
              " 'disappeared': tensor(0.0006),\n",
              " 'dived': tensor(0.0007),\n",
              " 'do': tensor(0.0008),\n",
              " 'does': tensor(0.0006),\n",
              " 'doesnt': tensor(0.0006),\n",
              " 'dog': tensor(0.0008),\n",
              " 'dogs': tensor(0.0006),\n",
              " 'doing': tensor(0.0008),\n",
              " 'don': tensor(0.0006),\n",
              " 'done': tensor(0.0006),\n",
              " 'dont': tensor(0.0007),\n",
              " 'door': tensor(0.0006),\n",
              " 'doors': tensor(0.0006),\n",
              " 'doorway': tensor(0.0006),\n",
              " 'dormitory': tensor(0.0007),\n",
              " 'down': tensor(0.0007),\n",
              " 'draco': tensor(0.0007),\n",
              " 'dragged': tensor(0.0007),\n",
              " 'dragon': tensor(0.0007),\n",
              " 'dragons': tensor(0.0006),\n",
              " 'dream': tensor(0.0008),\n",
              " 'dressed': tensor(0.0007),\n",
              " 'drew': tensor(0.0007),\n",
              " 'drills': tensor(0.0006),\n",
              " 'drink': tensor(0.0008),\n",
              " 'drive': tensor(0.0007),\n",
              " 'drop': tensor(0.0007),\n",
              " 'dropped': tensor(0.0006),\n",
              " 'drove': tensor(0.0007),\n",
              " 'dudley': tensor(0.0006),\n",
              " 'dudleys': tensor(0.0007),\n",
              " 'dumbledore': tensor(0.0008),\n",
              " 'dumbledores': tensor(0.0008),\n",
              " 'dungeons': tensor(0.0006),\n",
              " 'dunno': tensor(0.0008),\n",
              " 'during': tensor(0.0007),\n",
              " 'dursley': tensor(0.0006),\n",
              " 'dursleys': tensor(0.0005),\n",
              " 'each': tensor(0.0006),\n",
              " 'eagerly': tensor(0.0006),\n",
              " 'ear': tensor(0.0008),\n",
              " 'ears': tensor(0.0007),\n",
              " 'earth': tensor(0.0007),\n",
              " 'easily': tensor(0.0006),\n",
              " 'easy': tensor(0.0007),\n",
              " 'eat': tensor(0.0005),\n",
              " 'eating': tensor(0.0005),\n",
              " 'edge': tensor(0.0006),\n",
              " 'egg': tensor(0.0005),\n",
              " 'eh': tensor(0.0007),\n",
              " 'either': tensor(0.0006),\n",
              " 'eleven': tensor(0.0007),\n",
              " 'else': tensor(0.0006),\n",
              " 'em': tensor(0.0006),\n",
              " 'empty': tensor(0.0007),\n",
              " 'end': tensor(0.0006),\n",
              " 'enough': tensor(0.0007),\n",
              " 'entered': tensor(0.0006),\n",
              " 'entrance': tensor(0.0007),\n",
              " 'envelope': tensor(0.0007),\n",
              " 'er': tensor(0.0006),\n",
              " 'erised': tensor(0.0006),\n",
              " 'even': tensor(0.0008),\n",
              " 'evening': tensor(0.0007),\n",
              " 'ever': tensor(0.0006),\n",
              " 'every': tensor(0.0007),\n",
              " 'everybody': tensor(0.0006),\n",
              " 'everyone': tensor(0.0007),\n",
              " 'everything': tensor(0.0006),\n",
              " 'everywhere': tensor(0.0007),\n",
              " 'evil': tensor(0.0006),\n",
              " 'exactly': tensor(0.0006),\n",
              " 'exam': tensor(0.0006),\n",
              " 'exams': tensor(0.0007),\n",
              " 'excellent': tensor(0.0007),\n",
              " 'except': tensor(0.0006),\n",
              " 'excitedly': tensor(0.0007),\n",
              " 'excuse': tensor(0.0006),\n",
              " 'expect': tensor(0.0006),\n",
              " 'expected': tensor(0.0006),\n",
              " 'expelled': tensor(0.0006),\n",
              " 'explain': tensor(0.0007),\n",
              " 'extra': tensor(0.0006),\n",
              " 'eye': tensor(0.0008),\n",
              " 'eyes': tensor(0.0006),\n",
              " 'face': tensor(0.0008),\n",
              " 'faces': tensor(0.0006),\n",
              " 'facing': tensor(0.0007),\n",
              " 'fact': tensor(0.0007),\n",
              " 'faded': tensor(0.0007),\n",
              " 'fall': tensor(0.0006),\n",
              " 'fallen': tensor(0.0006),\n",
              " 'families': tensor(0.0007),\n",
              " 'family': tensor(0.0006),\n",
              " 'famous': tensor(0.0006),\n",
              " 'fang': tensor(0.0007),\n",
              " 'fangs': tensor(0.0008),\n",
              " 'far': tensor(0.0006),\n",
              " 'fast': tensor(0.0007),\n",
              " 'fat': tensor(0.0006),\n",
              " 'father': tensor(0.0006),\n",
              " 'fathers': tensor(0.0006),\n",
              " 'favorite': tensor(0.0007),\n",
              " 'fear': tensor(0.0008),\n",
              " 'feast': tensor(0.0008),\n",
              " 'feather': tensor(0.0007),\n",
              " 'feel': tensor(0.0005),\n",
              " 'feeling': tensor(0.0007),\n",
              " 'feet': tensor(0.0007),\n",
              " 'fell': tensor(0.0007),\n",
              " 'felt': tensor(0.0006),\n",
              " 'fer': tensor(0.0007),\n",
              " 'few': tensor(0.0007),\n",
              " 'field': tensor(0.0006),\n",
              " 'fifty': tensor(0.0006),\n",
              " 'fight': tensor(0.0007),\n",
              " 'fighting': tensor(0.0006),\n",
              " 'figure': tensor(0.0006),\n",
              " 'filch': tensor(0.0006),\n",
              " 'filled': tensor(0.0007),\n",
              " 'finally': tensor(0.0007),\n",
              " 'find': tensor(0.0006),\n",
              " 'finding': tensor(0.0006),\n",
              " 'fine': tensor(0.0007),\n",
              " 'fingers': tensor(0.0007),\n",
              " 'finished': tensor(0.0007),\n",
              " 'finnigan': tensor(0.0006),\n",
              " 'fire': tensor(0.0007),\n",
              " 'firenze': tensor(0.0008),\n",
              " 'firs': tensor(0.0006),\n",
              " 'first': tensor(0.0007),\n",
              " 'five': tensor(0.0006),\n",
              " 'fixed': tensor(0.0007),\n",
              " 'flamel': tensor(0.0007),\n",
              " 'flames': tensor(0.0006),\n",
              " 'flash': tensor(0.0007),\n",
              " 'flat': tensor(0.0006),\n",
              " 'flavor': tensor(0.0005),\n",
              " 'flew': tensor(0.0008),\n",
              " 'flint': tensor(0.0007),\n",
              " 'flitwick': tensor(0.0008),\n",
              " 'floating': tensor(0.0007),\n",
              " 'floor': tensor(0.0007),\n",
              " 'fluffy': tensor(0.0006),\n",
              " 'flute': tensor(0.0007),\n",
              " 'fly': tensor(0.0007),\n",
              " 'flying': tensor(0.0005),\n",
              " 'follow': tensor(0.0008),\n",
              " 'followed': tensor(0.0007),\n",
              " 'following': tensor(0.0006),\n",
              " 'food': tensor(0.0006),\n",
              " 'foot': tensor(0.0006),\n",
              " 'footsteps': tensor(0.0006),\n",
              " 'for': tensor(0.0007),\n",
              " 'forbidden': tensor(0.0008),\n",
              " 'force': tensor(0.0008),\n",
              " 'forehead': tensor(0.0008),\n",
              " 'forest': tensor(0.0007),\n",
              " 'forget': tensor(0.0007),\n",
              " 'forgotten': tensor(0.0006),\n",
              " 'forward': tensor(0.0006),\n",
              " 'found': tensor(0.0005),\n",
              " 'four': tensor(0.0006),\n",
              " 'fred': tensor(0.0007),\n",
              " 'free': tensor(0.0007),\n",
              " 'friend': tensor(0.0007),\n",
              " 'friends': tensor(0.0006),\n",
              " 'frog': tensor(0.0008),\n",
              " 'frogs': tensor(0.0007),\n",
              " 'from': tensor(0.0006),\n",
              " 'front': tensor(0.0007),\n",
              " 'full': tensor(0.0007),\n",
              " 'fun': tensor(0.0007),\n",
              " 'funny': tensor(0.0005),\n",
              " 'furious': tensor(0.0006),\n",
              " 'furiously': tensor(0.0006),\n",
              " 'game': tensor(0.0006),\n",
              " 'garden': tensor(0.0007),\n",
              " 'gasped': tensor(0.0007),\n",
              " 'gave': tensor(0.0007),\n",
              " 'gently': tensor(0.0006),\n",
              " 'george': tensor(0.0007),\n",
              " 'get': tensor(0.0006),\n",
              " 'gets': tensor(0.0006),\n",
              " 'gettin': tensor(0.0008),\n",
              " 'getting': tensor(0.0006),\n",
              " 'ghost': tensor(0.0007),\n",
              " 'ghosts': tensor(0.0007),\n",
              " 'giant': tensor(0.0007),\n",
              " 'girl': tensor(0.0005),\n",
              " 'girls': tensor(0.0006),\n",
              " 'give': tensor(0.0006),\n",
              " 'given': tensor(0.0006),\n",
              " 'giving': tensor(0.0006),\n",
              " 'glad': tensor(0.0007),\n",
              " 'glass': tensor(0.0007),\n",
              " 'glasses': tensor(0.0007),\n",
              " 'go': tensor(0.0006),\n",
              " 'goal': tensor(0.0008),\n",
              " 'goblin': tensor(0.0007),\n",
              " 'goblins': tensor(0.0007),\n",
              " 'goes': tensor(0.0008),\n",
              " 'going': tensor(0.0007),\n",
              " 'gold': tensor(0.0006),\n",
              " 'golden': tensor(0.0007),\n",
              " 'gone': tensor(0.0006),\n",
              " 'good': tensor(0.0006),\n",
              " 'goodbye': tensor(0.0007),\n",
              " 'got': tensor(0.0007),\n",
              " 'gotta': tensor(0.0006),\n",
              " 'gotten': tensor(0.0007),\n",
              " 'goyle': tensor(0.0007),\n",
              " 'grab': tensor(0.0007),\n",
              " 'grabbed': tensor(0.0007),\n",
              " 'granger': tensor(0.0007),\n",
              " 'grass': tensor(0.0006),\n",
              " 'gray': tensor(0.0006),\n",
              " 'great': tensor(0.0007),\n",
              " 'green': tensor(0.0007),\n",
              " 'grin': tensor(0.0008),\n",
              " 'gringotts': tensor(0.0006),\n",
              " 'griphook': tensor(0.0008),\n",
              " 'ground': tensor(0.0006),\n",
              " 'grounds': tensor(0.0006),\n",
              " 'growled': tensor(0.0007),\n",
              " 'grunted': tensor(0.0007),\n",
              " 'gryffindor': tensor(0.0007),\n",
              " 'gryffindors': tensor(0.0007),\n",
              " 'guard': tensor(0.0005),\n",
              " 'guarding': tensor(0.0006),\n",
              " 'h': tensor(0.0006),\n",
              " 'had': tensor(0.0006),\n",
              " 'hadnt': tensor(0.0006),\n",
              " 'hagrid': tensor(0.0006),\n",
              " 'hagrids': tensor(0.0005),\n",
              " 'hair': tensor(0.0006),\n",
              " 'half': tensor(0.0007),\n",
              " 'halfway': tensor(0.0007),\n",
              " 'hall': tensor(0.0007),\n",
              " 'halloween': tensor(0.0007),\n",
              " 'hand': tensor(0.0006),\n",
              " 'handed': tensor(0.0006),\n",
              " 'handle': tensor(0.0007),\n",
              " 'hands': tensor(0.0007),\n",
              " 'hang': tensor(0.0007),\n",
              " 'hanging': tensor(0.0007),\n",
              " 'happen': tensor(0.0007),\n",
              " 'happened': tensor(0.0008),\n",
              " 'happy': tensor(0.0006),\n",
              " 'hard': tensor(0.0007),\n",
              " 'harder': tensor(0.0006),\n",
              " 'hardly': tensor(0.0006),\n",
              " 'harry': tensor(0.0019),\n",
              " 'harrys': tensor(0.0006),\n",
              " 'has': tensor(0.0006),\n",
              " 'hasnt': tensor(0.0007),\n",
              " 'hat': tensor(0.0007),\n",
              " 'hate': tensor(0.0005),\n",
              " 'hated': tensor(0.0006),\n",
              " 'have': tensor(0.0007),\n",
              " 'havent': tensor(0.0008),\n",
              " 'having': tensor(0.0007),\n",
              " 'he': tensor(0.0007),\n",
              " 'head': tensor(0.0005),\n",
              " 'headless': tensor(0.0007),\n",
              " 'heads': tensor(0.0007),\n",
              " 'hear': tensor(0.0007),\n",
              " 'heard': tensor(0.0006),\n",
              " 'heart': tensor(0.0006),\n",
              " 'heavy': tensor(0.0006),\n",
              " 'hed': tensor(0.0007),\n",
              " 'hedwig': tensor(0.0005),\n",
              " 'held': tensor(0.0006),\n",
              " 'hell': tensor(0.0006),\n",
              " 'help': tensor(0.0006),\n",
              " 'her': tensor(0.0006),\n",
              " 'here': tensor(0.0006),\n",
              " 'hermione': tensor(0.0006),\n",
              " 'hermiones': tensor(0.0007),\n",
              " 'herself': tensor(0.0007),\n",
              " 'hes': tensor(0.0007),\n",
              " 'hidden': tensor(0.0008),\n",
              " 'hide': tensor(0.0007),\n",
              " 'hiding': tensor(0.0006),\n",
              " 'high': tensor(0.0006),\n",
              " 'higher': tensor(0.0006),\n",
              " 'him': tensor(0.0007),\n",
              " 'himself': tensor(0.0007),\n",
              " 'his': tensor(0.0008),\n",
              " 'hissed': tensor(0.0007),\n",
              " 'history': tensor(0.0006),\n",
              " 'hit': tensor(0.0007),\n",
              " 'hogwarts': tensor(0.0007),\n",
              " 'hold': tensor(0.0007),\n",
              " 'holding': tensor(0.0007),\n",
              " 'hole': tensor(0.0007),\n",
              " 'holidays': tensor(0.0006),\n",
              " 'home': tensor(0.0007),\n",
              " 'homework': tensor(0.0006),\n",
              " 'honestly': tensor(0.0008),\n",
              " 'hooch': tensor(0.0006),\n",
              " 'hoops': tensor(0.0007),\n",
              " 'hope': tensor(0.0007),\n",
              " 'hoping': tensor(0.0007),\n",
              " 'horrible': tensor(0.0007),\n",
              " 'horror': tensor(0.0008),\n",
              " 'hospital': tensor(0.0007),\n",
              " 'hot': tensor(0.0006),\n",
              " 'hour': tensor(0.0007),\n",
              " 'hours': tensor(0.0006),\n",
              " 'house': tensor(0.0007),\n",
              " 'houses': tensor(0.0006),\n",
              " 'how': tensor(0.0005),\n",
              " 'however': tensor(0.0006),\n",
              " 'howling': tensor(0.0007),\n",
              " 'hufflepuff': tensor(0.0007),\n",
              " 'huge': tensor(0.0007),\n",
              " 'human': tensor(0.0007),\n",
              " 'hundred': tensor(0.0008),\n",
              " 'hundreds': tensor(0.0006),\n",
              " 'hung': tensor(0.0006),\n",
              " 'hungry': tensor(0.0007),\n",
              " 'hurried': tensor(0.0007),\n",
              " 'hurry': tensor(0.0005),\n",
              " 'hurrying': tensor(0.0005),\n",
              " 'hurt': tensor(0.0006),\n",
              " 'hut': tensor(0.0008),\n",
              " 'i': tensor(0.0007),\n",
              " 'ice': tensor(0.0006),\n",
              " 'id': tensor(0.0006),\n",
              " 'idea': tensor(0.0006),\n",
              " 'if': tensor(0.0006),\n",
              " 'ignored': tensor(0.0006),\n",
              " 'ill': tensor(0.0006),\n",
              " 'im': tensor(0.0007),\n",
              " 'imagine': tensor(0.0007),\n",
              " 'important': tensor(0.0006),\n",
              " 'in': tensor(0.0006),\n",
              " 'inches': tensor(0.0007),\n",
              " 'indeed': tensor(0.0007),\n",
              " 'inside': tensor(0.0007),\n",
              " 'instead': tensor(0.0007),\n",
              " 'interested': tensor(0.0007),\n",
              " 'interesting': tensor(0.0006),\n",
              " 'into': tensor(0.0006),\n",
              " 'invisibility': tensor(0.0006),\n",
              " 'invisible': tensor(0.0006),\n",
              " 'is': tensor(0.0005),\n",
              " 'isnt': tensor(0.0006),\n",
              " 'it': tensor(0.0007),\n",
              " 'itll': tensor(0.0006),\n",
              " 'its': tensor(0.0007),\n",
              " 'itself': tensor(0.0006),\n",
              " 'ive': tensor(0.0007),\n",
              " 'jerked': tensor(0.0006),\n",
              " 'job': tensor(0.0007),\n",
              " 'join': tensor(0.0007),\n",
              " 'joined': tensor(0.0006),\n",
              " 'joke': tensor(0.0006),\n",
              " 'jordan': tensor(0.0006),\n",
              " 'jump': tensor(0.0008),\n",
              " 'jumped': tensor(0.0007),\n",
              " 'jus': tensor(0.0007),\n",
              " 'just': tensor(0.0007),\n",
              " 'keep': tensor(0.0006),\n",
              " 'keeper': tensor(0.0007),\n",
              " 'keeping': tensor(0.0006),\n",
              " 'kept': tensor(0.0006),\n",
              " 'key': tensor(0.0007),\n",
              " 'keys': tensor(0.0006),\n",
              " 'kicked': tensor(0.0007),\n",
              " 'kill': tensor(0.0007),\n",
              " 'killed': tensor(0.0007),\n",
              " 'kind': tensor(0.0007),\n",
              " 'kings': tensor(0.0006),\n",
              " 'kitchen': tensor(0.0006),\n",
              " 'knees': tensor(0.0008),\n",
              " 'knew': tensor(0.0006),\n",
              " 'knight': tensor(0.0006),\n",
              " 'knock': tensor(0.0006),\n",
              " 'knocked': tensor(0.0006),\n",
              " 'knocking': tensor(0.0006),\n",
              " 'know': tensor(0.0007),\n",
              " 'knowing': tensor(0.0007),\n",
              " 'known': tensor(0.0006),\n",
              " 'knows': tensor(0.0007),\n",
              " 'knuts': tensor(0.0008),\n",
              " 'lady': tensor(0.0007),\n",
              " 'lake': tensor(0.0006),\n",
              " 'lamp': tensor(0.0007),\n",
              " 'landed': tensor(0.0007),\n",
              " 'large': tensor(0.0006),\n",
              " 'last': tensor(0.0008),\n",
              " 'late': tensor(0.0007),\n",
              " 'later': tensor(0.0006),\n",
              " 'laugh': tensor(0.0007),\n",
              " 'laughed': tensor(0.0007),\n",
              " 'laughing': tensor(0.0008),\n",
              " 'laughter': tensor(0.0008),\n",
              " 'lay': tensor(0.0006),\n",
              " 'lead': tensor(0.0008),\n",
              " 'leading': tensor(0.0006),\n",
              " 'leaky': tensor(0.0006),\n",
              " 'leaned': tensor(0.0007),\n",
              " 'leapt': tensor(0.0007),\n",
              " 'learn': tensor(0.0006),\n",
              " 'learned': tensor(0.0005),\n",
              " 'least': tensor(0.0006),\n",
              " 'leave': tensor(0.0007),\n",
              " 'leaves': tensor(0.0006),\n",
              " 'leaving': tensor(0.0007),\n",
              " 'led': tensor(0.0006),\n",
              " 'lee': tensor(0.0007),\n",
              " 'left': tensor(0.0006),\n",
              " 'leg': tensor(0.0006),\n",
              " 'legs': tensor(0.0006),\n",
              " 'lemon': tensor(0.0007),\n",
              " 'less': tensor(0.0007),\n",
              " 'lesson': tensor(0.0007),\n",
              " 'lessons': tensor(0.0008),\n",
              " 'let': tensor(0.0007),\n",
              " 'lets': tensor(0.0006),\n",
              " 'letter': tensor(0.0006),\n",
              " 'letters': tensor(0.0007),\n",
              " 'library': tensor(0.0007),\n",
              " 'lie': tensor(0.0006),\n",
              " 'life': tensor(0.0006),\n",
              " 'light': tensor(0.0007),\n",
              " 'lightning': tensor(0.0006),\n",
              " 'like': tensor(0.0008),\n",
              " 'liked': tensor(0.0008),\n",
              " 'lily': tensor(0.0007),\n",
              " 'line': tensor(0.0007),\n",
              " 'lips': tensor(0.0007),\n",
              " 'list': tensor(0.0007),\n",
              " 'listen': tensor(0.0007),\n",
              " 'listening': tensor(0.0008),\n",
              " 'lit': tensor(0.0006),\n",
              " 'little': tensor(0.0008),\n",
              " 'live': tensor(0.0006),\n",
              " 'lived': tensor(0.0007),\n",
              " 'living': tensor(0.0007),\n",
              " 'loads': tensor(0.0006),\n",
              " 'lock': tensor(0.0008),\n",
              " 'locked': tensor(0.0007),\n",
              " 'london': tensor(0.0008),\n",
              " 'long': tensor(0.0007),\n",
              " 'longbottom': tensor(0.0007),\n",
              " 'look': tensor(0.0006),\n",
              " 'looked': tensor(0.0007),\n",
              " 'looking': tensor(0.0006),\n",
              " 'looks': tensor(0.0006),\n",
              " 'lose': tensor(0.0007),\n",
              " 'losing': tensor(0.0007),\n",
              " 'lost': tensor(0.0008),\n",
              " 'lot': tensor(0.0007),\n",
              " 'lots': tensor(0.0006),\n",
              " 'loud': tensor(0.0007),\n",
              " 'loudly': tensor(0.0007),\n",
              " 'low': tensor(0.0007),\n",
              " 'luck': tensor(0.0006),\n",
              " 'lucky': tensor(0.0008),\n",
              " 'lumpy': tensor(0.0007),\n",
              " 'lurking': tensor(0.0007),\n",
              " 'lying': tensor(0.0009),\n",
              " 'mad': tensor(0.0007),\n",
              " 'madam': tensor(0.0007),\n",
              " 'made': tensor(0.0008),\n",
              " 'magic': tensor(0.0006),\n",
              " 'magical': tensor(0.0007),\n",
              " 'mail': tensor(0.0007),\n",
              " 'make': tensor(0.0007),\n",
              " 'making': tensor(0.0007),\n",
              " 'malfoy': tensor(0.0006),\n",
              " 'malfoys': tensor(0.0006),\n",
              " 'man': tensor(0.0007),\n",
              " 'managed': tensor(0.0006),\n",
              " 'many': tensor(0.0007),\n",
              " 'marble': tensor(0.0006),\n",
              " 'marched': tensor(0.0007),\n",
              " 'match': tensor(0.0007),\n",
              " 'matter': tensor(0.0008),\n",
              " 'may': tensor(0.0008),\n",
              " 'maybe': tensor(0.0006),\n",
              " 'mcgonagall': tensor(0.0007),\n",
              " 'mcgonagalls': tensor(0.0006),\n",
              " 'me': tensor(0.0007),\n",
              " 'mean': tensor(0.0006),\n",
              " 'means': tensor(0.0007),\n",
              " 'meant': tensor(0.0006),\n",
              " 'meet': tensor(0.0007),\n",
              " 'mention': tensor(0.0006),\n",
              " 'met': tensor(0.0007),\n",
              " 'midair': tensor(0.0007),\n",
              " 'middle': tensor(0.0006),\n",
              " 'midnight': tensor(0.0008),\n",
              " 'might': tensor(0.0006),\n",
              " 'mind': tensor(0.0005),\n",
              " 'ministry': tensor(0.0007),\n",
              " 'minute': tensor(0.0006),\n",
              " 'minutes': tensor(0.0006),\n",
              " 'mirror': tensor(0.0005),\n",
              " 'miss': tensor(0.0006),\n",
              " 'mistake': tensor(0.0007),\n",
              " 'moaned': tensor(0.0006),\n",
              " 'mom': tensor(0.0007),\n",
              " 'moment': tensor(0.0005),\n",
              " 'money': tensor(0.0007),\n",
              " 'moonlight': tensor(0.0007),\n",
              " 'more': tensor(0.0006),\n",
              " 'morning': tensor(0.0007),\n",
              " 'most': tensor(0.0007),\n",
              " 'mother': tensor(0.0007),\n",
              " 'mothers': tensor(0.0006),\n",
              " 'motorcycle': tensor(0.0006),\n",
              " 'mountain': tensor(0.0007),\n",
              " 'mouth': tensor(0.0007),\n",
              " 'move': tensor(0.0007),\n",
              " 'moved': tensor(0.0006),\n",
              " 'moving': tensor(0.0006),\n",
              " 'mr': tensor(0.0006),\n",
              " 'mrs': tensor(0.0007),\n",
              " 'much': tensor(0.0006),\n",
              " 'muggle': tensor(0.0008),\n",
              " 'muggles': tensor(0.0008),\n",
              " 'murmured': tensor(0.0007),\n",
              " 'must': tensor(0.0006),\n",
              " 'mustache': tensor(0.0007),\n",
              " 'mustve': tensor(0.0007),\n",
              " 'muttered': tensor(0.0007),\n",
              " 'muttering': tensor(0.0008),\n",
              " 'my': tensor(0.0005),\n",
              " 'myself': tensor(0.0006),\n",
              " 'mysterious': tensor(0.0007),\n",
              " 'nah': tensor(0.0007),\n",
              " 'name': tensor(0.0008),\n",
              " 'names': tensor(0.0006),\n",
              " 'narrow': tensor(0.0007),\n",
              " 'nasty': tensor(0.0008),\n",
              " 'near': tensor(0.0007),\n",
              " 'nearer': tensor(0.0007),\n",
              " 'nearest': tensor(0.0006),\n",
              " 'nearly': tensor(0.0006),\n",
              " 'neck': tensor(0.0006),\n",
              " 'need': tensor(0.0006),\n",
              " 'needed': tensor(0.0008),\n",
              " 'needs': tensor(0.0006),\n",
              " 'neither': tensor(0.0006),\n",
              " 'nervous': tensor(0.0006),\n",
              " 'nervously': tensor(0.0005),\n",
              " 'never': tensor(0.0007),\n",
              " 'neville': tensor(0.0006),\n",
              " 'nevilles': tensor(0.0008),\n",
              " 'new': tensor(0.0006),\n",
              " 'news': tensor(0.0006),\n",
              " 'newspaper': tensor(0.0006),\n",
              " 'next': tensor(0.0007),\n",
              " 'nice': tensor(0.0006),\n",
              " 'nicolas': tensor(0.0007),\n",
              " 'night': tensor(0.0006),\n",
              " 'nimbus': tensor(0.0008),\n",
              " 'nine': tensor(0.0007),\n",
              " 'no': tensor(0.0007),\n",
              " 'nobody': tensor(0.0008),\n",
              " 'nodded': tensor(0.0006),\n",
              " 'noise': tensor(0.0005),\n",
              " 'none': tensor(0.0007),\n",
              " 'nor': tensor(0.0006),\n",
              " 'norbert': tensor(0.0007),\n",
              " 'normal': tensor(0.0006),\n",
              " 'norris': tensor(0.0008),\n",
              " 'nose': tensor(0.0007),\n",
              " 'noses': tensor(0.0007),\n",
              " 'nostrils': tensor(0.0007),\n",
              " 'not': tensor(0.0007),\n",
              " 'note': tensor(0.0006),\n",
              " 'notes': tensor(0.0007),\n",
              " 'nothin': tensor(0.0007),\n",
              " 'nothing': tensor(0.0007),\n",
              " 'notice': tensor(0.0006),\n",
              " 'noticed': tensor(0.0006),\n",
              " 'noticing': tensor(0.0007),\n",
              " 'now': tensor(0.0006),\n",
              " 'number': tensor(0.0006),\n",
              " 'o': tensor(0.0007),\n",
              " 'obviously': tensor(0.0007),\n",
              " 'oclock': tensor(0.0007),\n",
              " 'odd': tensor(0.0007),\n",
              " 'of': tensor(0.0007),\n",
              " 'off': tensor(0.0006),\n",
              " 'often': tensor(0.0006),\n",
              " 'oh': tensor(0.0006),\n",
              " 'old': tensor(0.0007),\n",
              " 'older': tensor(0.0006),\n",
              " 'ollivander': tensor(0.0006),\n",
              " 'on': tensor(0.0007),\n",
              " 'once': tensor(0.0007),\n",
              " 'one': tensor(0.0006),\n",
              " 'ones': tensor(0.0007),\n",
              " 'only': tensor(0.0005),\n",
              " 'onto': tensor(0.0007),\n",
              " 'open': tensor(0.0005),\n",
              " 'opened': tensor(0.0006),\n",
              " 'opposite': tensor(0.0007),\n",
              " 'or': tensor(0.0008),\n",
              " 'ordinary': tensor(0.0006),\n",
              " 'other': tensor(0.0007),\n",
              " 'others': tensor(0.0006),\n",
              " 'our': tensor(0.0007),\n",
              " 'out': tensor(0.0006),\n",
              " 'outside': tensor(0.0007),\n",
              " 'outta': tensor(0.0006),\n",
              " 'over': tensor(0.0007),\n",
              " 'overhead': tensor(0.0006),\n",
              " 'owl': tensor(0.0006),\n",
              " 'owls': tensor(0.0006),\n",
              " 'own': tensor(0.0005),\n",
              " 'pack': tensor(0.0007),\n",
              " 'package': tensor(0.0006),\n",
              " 'packed': tensor(0.0007),\n",
              " 'pain': tensor(0.0006),\n",
              " 'pair': tensor(0.0008),\n",
              " 'pale': tensor(0.0007),\n",
              " 'panted': tensor(0.0007),\n",
              " 'paper': tensor(0.0006),\n",
              " 'parcel': tensor(0.0008),\n",
              " 'parchment': tensor(0.0007),\n",
              " 'parents': tensor(0.0008),\n",
              " 'particularly': tensor(0.0007),\n",
              " 'passageway': tensor(0.0007),\n",
              " 'passed': tensor(0.0006),\n",
              " 'passing': tensor(0.0006),\n",
              " 'past': tensor(0.0007),\n",
              " 'path': tensor(0.0008),\n",
              " 'patil': tensor(0.0006),\n",
              " 'peered': tensor(0.0006),\n",
              " 'peering': tensor(0.0007),\n",
              " 'peeves': tensor(0.0007),\n",
              " 'people': tensor(0.0006),\n",
              " 'percy': tensor(0.0006),\n",
              " 'perfect': tensor(0.0006),\n",
              " 'perhaps': tensor(0.0008),\n",
              " 'person': tensor(0.0007),\n",
              " 'petunia': tensor(0.0006),\n",
              " 'picked': tensor(0.0007),\n",
              " 'piece': tensor(0.0008),\n",
              " 'pieces': tensor(0.0007),\n",
              " 'piers': tensor(0.0006),\n",
              " 'pig': tensor(0.0007),\n",
              " 'pile': tensor(0.0007),\n",
              " 'piled': tensor(0.0007),\n",
              " 'pink': tensor(0.0007),\n",
              " 'pinned': tensor(0.0007),\n",
              " 'place': tensor(0.0008),\n",
              " 'planets': tensor(0.0008),\n",
              " 'plant': tensor(0.0007),\n",
              " 'plates': tensor(0.0006),\n",
              " 'platform': tensor(0.0007),\n",
              " 'platforms': tensor(0.0007),\n",
              " 'play': tensor(0.0006),\n",
              " 'players': tensor(0.0005),\n",
              " 'playing': tensor(0.0006),\n",
              " 'please': tensor(0.0007),\n",
              " 'pleased': tensor(0.0006),\n",
              " 'pocket': tensor(0.0006),\n",
              " 'pockets': tensor(0.0007),\n",
              " 'point': tensor(0.0007),\n",
              " 'pointed': tensor(0.0006),\n",
              " 'pointing': tensor(0.0007),\n",
              " 'points': tensor(0.0006),\n",
              " 'pomfrey': tensor(0.0006),\n",
              " 'poor': tensor(0.0006),\n",
              " 'portrait': tensor(0.0008),\n",
              " 'possession': tensor(0.0006),\n",
              " 'possible': tensor(0.0007),\n",
              " 'posts': tensor(0.0007),\n",
              " 'potion': tensor(0.0007),\n",
              " 'potions': tensor(0.0006),\n",
              " 'potter': tensor(0.0007),\n",
              " 'potters': tensor(0.0007),\n",
              " 'power': tensor(0.0006),\n",
              " 'powerful': tensor(0.0008),\n",
              " 'practice': tensor(0.0007),\n",
              " 'prefect': tensor(0.0006),\n",
              " 'prefects': tensor(0.0006),\n",
              " 'presents': tensor(0.0007),\n",
              " 'pressed': tensor(0.0008),\n",
              " 'privet': tensor(0.0008),\n",
              " 'probably': tensor(0.0008),\n",
              " 'professor': tensor(0.0007),\n",
              " 'properly': tensor(0.0006),\n",
              " 'proud': tensor(0.0006),\n",
              " 'pull': tensor(0.0007),\n",
              " 'pulled': tensor(0.0006),\n",
              " 'pulling': tensor(0.0006),\n",
              " 'purple': tensor(0.0006),\n",
              " 'pushed': tensor(0.0006),\n",
              " 'put': tensor(0.0006),\n",
              " 'quaffle': tensor(0.0007),\n",
              " 'question': tensor(0.0006),\n",
              " 'questions': tensor(0.0007),\n",
              " 'quick': tensor(0.0006),\n",
              " 'quickly': tensor(0.0006),\n",
              " 'quidditch': tensor(0.0005),\n",
              " 'quiet': tensor(0.0006),\n",
              " 'quietly': tensor(0.0006),\n",
              " 'quills': tensor(0.0007),\n",
              " 'quirrell': tensor(0.0007),\n",
              " 'quirrells': tensor(0.0007),\n",
              " 'quite': tensor(0.0006),\n",
              " 'racing': tensor(0.0006),\n",
              " 'raised': tensor(0.0006),\n",
              " 'ran': tensor(0.0006),\n",
              " 'rang': tensor(0.0006),\n",
              " 'rat': tensor(0.0008),\n",
              " 'rather': tensor(0.0006),\n",
              " 'ravenclaw': tensor(0.0007),\n",
              " 'reached': tensor(0.0006),\n",
              " 'read': tensor(0.0006),\n",
              " 'ready': tensor(0.0006),\n",
              " 'real': tensor(0.0006),\n",
              " 'realize': tensor(0.0008),\n",
              " 'realized': tensor(0.0007),\n",
              " 'really': tensor(0.0007),\n",
              " 'reason': tensor(0.0006),\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "from math import exp\n",
        "word_exp_dict = {}\n",
        "for word, dot_value in word_dot_dict.items():\n",
        "  exp_value = torch.exp(dot_value)\n",
        "  word_exp_dict[word] = exp_value\n",
        "word_exp_dict\n",
        "\n",
        "sum_exp = sum([value for value in word_exp_dict.values()])\n",
        "\n",
        "word_prob_dict = dict()\n",
        "for word, exp_value in word_exp_dict.items():\n",
        "  word_prob_dict[word] = exp_value / sum_exp\n",
        "word_prob_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpXH7RxSbwUO",
        "outputId": "6f7c4262-0f73-4b44-f1f2-5b0c0137ff2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0007)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Get P(potter|harry)\n",
        "word_prob_dict['potter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxz9SJhSuhy"
      },
      "source": [
        "## 13. Efficient Matrix Operations\n",
        "![img](https://mkang32.github.io/images/python/khan_academy_matrix_product.png)\n",
        "\n",
        "Instead of calculating dot products one by one, we can use matrix multiplication for efficiency:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "harry, potter\n",
        "center_word_mat = torch.stack([harry , potter])\n",
        "center_word_mat.shape"
      ],
      "metadata": {
        "id": "MpfW83rXBN5v",
        "outputId": "1edd3cbb-bea0-48d6-9661-f131af2f566a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCFajiDkb44W",
        "outputId": "2df0aae3-c15c-4ac7-ca8b-141188e5423c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1506, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# get dot product result for every word in the vocabulary\n",
        "harry.shape\n",
        "# first, make vector_of_harry into matrix format\n",
        "harry_mat = harry.unsqueeze(0)\n",
        "word_vectors.shape\n",
        "# do matrix multiplication\n",
        "dot_by_mat = torch.mm(center_word_mat, word_vectors.T)\n",
        "dot_by_mat = dot_by_mat.T\n",
        "dot_by_mat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrOLs6kgyhDY"
      },
      "source": [
        "Let's verify that our matrix multiplication gives the same result as individual dot products:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf643k38yhDY",
        "outputId": "b1948bd6-a1cd-4d40-e3a0-ddfa0940d151"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0024, 1.1192]), tensor(0.0024))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "dot_by_mat[word2idx['potter']] , word_dot_dict['potter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7DIzDCpyhDY"
      },
      "source": [
        "Now let's implement the complete softmax calculation using matrix operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5USdps5CfKzq",
        "outputId": "1be201cf-57d7-4fef-e10b-582527c08100"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1506, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# convert dot product result into exponential\n",
        "mat_exp = torch.exp(dot_by_mat)\n",
        "mat_exp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNq0gqz0fntB",
        "outputId": "ebb5bcfa-ab8e-475b-858c-2494d5c531cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1518.9807, 1514.7291])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "# get the sum of exponential\n",
        "sum(mat_exp)\n",
        "sum_of_mat_exp = torch.sum(mat_exp, dim=0)\n",
        "sum_of_mat_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruV4_myDgLvG",
        "outputId": "66b722e0-7412-45fc-c9ec-e1bfa9d90c2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# divide exponential value with sum\n",
        "prob = mat_exp / sum_of_mat_exp\n",
        "prob.sum(dim = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRfw7T5nyhDY"
      },
      "source": [
        "## 14. Creating a Probability Function\n",
        "\n",
        "Let's create a function to calculate probabilities efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF4gutuEgntj",
        "outputId": "03e42817-e047-41fd-c6e9-70291904bbde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0007, 0.0006],\n",
              "        [0.0007, 0.0007],\n",
              "        [0.0006, 0.0007],\n",
              "        ...,\n",
              "        [0.0006, 0.0006],\n",
              "        [0.0007, 0.0007],\n",
              "        [0.0008, 0.0006]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "def get_probs(query_vectors, entire_vectors):\n",
        "  dot_by_mat = torch.mm(query_vectors, entire_vectors.T)\n",
        "  dot_by_mat = dot_by_mat.T\n",
        "  mat_exp = torch.exp(dot_by_mat)\n",
        "  sum_of_mat_exp = torch.sum(mat_exp, dim=0)\n",
        "  prob = mat_exp / sum_of_mat_exp\n",
        "  return prob\n",
        "\n",
        "get_probs(center_word_mat, word_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOo5uSXbyhDY"
      },
      "source": [
        "## 15. Preparing for Training\n",
        "\n",
        "Before training our Word2Vec model, we need to split our dataset into training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gjCSY3DoaFg",
        "outputId": "b40366c4-d819-42f1-c5c4-f80084d4e80e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226846\n"
          ]
        }
      ],
      "source": [
        "# Now we can train the word2vec\n",
        "import random\n",
        "# Let's think about training pairs\n",
        "index_pairs # this is our dataset. It's list of list of two integer\n",
        "print(len(index_pairs))\n",
        "# two integer means a pair of neighboring words\n",
        "\n",
        "# Training set and Test set\n",
        "# To validate that our model can solve 'unseen' problems\n",
        "# So we have to split the dataset before training.\n",
        "\n",
        "# To randomly split the dataset, we will first shuffle the dataset\n",
        "\n",
        "random.shuffle(index_pairs) # this will shuffle the list items"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = index_pairs[:200000]\n",
        "test_set = index_pairs[200000:]"
      ],
      "metadata": {
        "id": "jNzrs1JQEYiB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC0QF3NZp9Rh",
        "outputId": "d5de2037-c8fb-48e7-a2e4-abb530f3e54d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 26846)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "len(train_set), len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKMeat2TyhDZ"
      },
      "source": [
        "## 16. Training the Word2Vec Model\n",
        "\n",
        "Now we'll train our Word2Vec model using batched gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpUM3aiiqG8e",
        "outputId": "d9dfc7cd-ac42-4ead-f070-4ab7a2a207a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1506, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# making batch from train_set\n",
        "# Batch is a set of training samples, that are calculated together\n",
        "# And also we update the model after one single batch\n",
        "\n",
        "batch = train_set[:20]\n",
        "batch\n",
        "center_words = [x[0] for x in batch]\n",
        "context_words = [x[1] for x in batch]\n",
        "\n",
        "center_words_vectors = word_vectors[center_words]\n",
        "prob = get_probs(center_words_vectors, word_vectors)\n",
        "prob.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "center_words_vectors.shape\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(center_words_vectors)"
      ],
      "metadata": {
        "id": "SxeY7VDZpIqQ",
        "outputId": "ddbbf070-53d5-49f8-ecae-a72858bb8982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a0cd20c9210>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training of torch\n",
        "- Divide it in batch level\n",
        "- for each batch:\n",
        "  - calculate model prediction\n",
        "  - calculate the loss\n",
        "  - backprop the loss\n",
        "  - update the parameters using gradient\n",
        "  - repeat"
      ],
      "metadata": {
        "id": "r8xfDstuFYYO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2frfncyyhDZ"
      },
      "source": [
        "## 17. Evaluating the Training\n",
        "\n",
        "Let's visualize the training loss to see if our model is learning:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob = get_probs(center_words_vectors, entire_vectors = word_vectors)\n",
        "print(prob[:,0])\n",
        "\n",
        "context_words"
      ],
      "metadata": {
        "id": "i1FBAKCcqNV3",
        "outputId": "6a87c0aa-1a05-4244-865d-8c3f2207f01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0006, 0.0007, 0.0008,  ..., 0.0006, 0.0007, 0.0007])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1212,\n",
              " 483,\n",
              " 383,\n",
              " 206,\n",
              " 872,\n",
              " 1264,\n",
              " 704,\n",
              " 1264,\n",
              " 1266,\n",
              " 1266,\n",
              " 702,\n",
              " 1035,\n",
              " 381,\n",
              " 986,\n",
              " 1494,\n",
              " 534,\n",
              " 33,\n",
              " 563,\n",
              " 939,\n",
              " 1398]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "FJHSV8zbyLYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "293e5aa8-37a1-48b8-8fa5-d977bcf152f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loss_record' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-2a222cc38d8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_record' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tZxnhPdyhDZ"
      },
      "source": [
        "## 18. Testing the Model\n",
        "\n",
        "Now we'll test our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDP9aR48zdJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPgR_2dyhDZ"
      },
      "source": [
        "## 19. Exploring Learned Word Relationships\n",
        "\n",
        "Let's explore what our model has learned by finding the words most closely related to \"harry\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YaCDNq_0hFc"
      },
      "outputs": [],
      "source": [
        "# P(potter|harry)?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}