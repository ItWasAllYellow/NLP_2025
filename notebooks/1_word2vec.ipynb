{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItWasAllYellow/NLP_2025/blob/main/notebooks/1_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSdE5ikj0SwV"
      },
      "source": [
        "# Word2Vec Implementation from Scratch\n",
        "\n",
        "This notebook demonstrates how to implement the Word2Vec algorithm from scratch using PyTorch. We'll use the first Harry Potter book as our corpus to train word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Egm0weh0SwY"
      },
      "source": [
        "## 1. Setting Up the Environment\n",
        "\n",
        "First, we need to import the necessary libraries:\n",
        "- `torch` and `torch.nn` for tensor operations and neural network functionality\n",
        "- `string` for string manipulations (removing punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L9BA5Lg2QRMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HElxifl0Swb"
      },
      "source": [
        "## 2. Getting the Text Data\n",
        "\n",
        "We'll download the first Harry Potter book to use as our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEaaz_s0QRMs",
        "outputId": "f76a5088-07b6-45ce-86b2-8db3ec1af872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-13 05:28:42--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt.1’\n",
            "\n",
            "\r          J. K. Row   0%[                    ]       0  --.-KB/s               \rJ. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-13 05:28:42 (8.85 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt.1’ saved [439742/439742]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcHGwy00Swb"
      },
      "source": [
        "## 3. Text Preprocessing\n",
        "\n",
        "Before we can use the text data, we need to preprocess it:\n",
        "- Remove punctuation\n",
        "- Convert text to lowercase\n",
        "- Split text into tokens (words)\n",
        "\n",
        "This function will help us clean and tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CUsXJYlIQRMs"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(x):\n",
        "  return x.translate(''.maketrans('', '', string.punctuation))\n",
        "\n",
        "def make_tokenized_corpus(corpus):\n",
        "  out= [ [y.lower() for y in remove_punctuation(sentence).split(' ') if y] for sentence in corpus]\n",
        "  return [x for x in out if x!=[]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk2WMdfK0Swc"
      },
      "source": [
        "## 4. Loading and Formatting the Text\n",
        "\n",
        "Now we'll load the text file, replace some special characters, and split the text into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ry1o-F-bQRMs"
      },
      "outputs": [],
      "source": [
        "with open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'r') as f:\n",
        "  strings = f.readlines()\n",
        "sample_text = \"\".join(strings).replace('\\n', ' ').replace('Mr.', 'mr').replace('Mrs.', 'mrs').split('. ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF8Wcj5R0Swc"
      },
      "source": [
        "Let's tokenize the text using our preprocessing function `make_tokenized_corpus`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ne-pUaxSQRMs"
      },
      "outputs": [],
      "source": [
        "# Corpus is a list of list of strings (words)\n",
        "corpus = make_tokenized_corpus(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpb--36B0Swd"
      },
      "source": [
        "## 5. Creating Context Word Pairs\n",
        "\n",
        "A key concept in Word2Vec is learning from context. We need to create pairs of words that appear near each other in the text. We'll use a sliding window approach to create these pairs.\n",
        "\n",
        "For example, with the window size of 2, for the word \"to\" in the sentence \"they were the last people youd expect to be involved...\", we would create pairs with:\n",
        "- (\"to\", \"expect\")\n",
        "- (\"to\", \"be\")\n",
        "- (\"to\", \"involved\")\n",
        "- (\"to\", \"in\")\n",
        "\n",
        "These pairs will be our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UxJwTAacWfP",
        "outputId": "05e2ff79-2b4f-492c-c2be-2efec072ed15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4682/4682 [00:00<00:00, 8004.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Length of word_pairs is 282372\n",
            "First 5 example of word_pairs is [('harry', 'potter'), ('harry', 'and'), ('potter', 'harry'), ('potter', 'and'), ('potter', 'the')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "sample_sentence = ['they', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense']\n",
        "\n",
        "word_pairs = []\n",
        "window_size = 2\n",
        "\n",
        "for sample_sentence in tqdm(corpus):\n",
        "    for curr_idx, center_word in enumerate(sample_sentence):\n",
        "        #print(curr_idx, center_word)\n",
        "\n",
        "        window_begin = max(curr_idx - window_size, 0)\n",
        "        window_end = min(curr_idx + window_size + 1, len(sample_sentence))\n",
        "\n",
        "        #for context_word in sample_sentence[window_begin : window_end]:\n",
        "            #if center_word == context_word: continue -> context word와 주변 단어가 같은 경우도 존재하므로 문제가 됨\n",
        "\n",
        "            #word_pairs.append((center_word, context_word))\n",
        "\n",
        "        for j in range(window_begin, window_end):\n",
        "            if curr_idx == j: continue\n",
        "            word_pairs.append((center_word, sample_sentence[j]))\n",
        "\n",
        "print(f\"\\nLength of word_pairs is {len(word_pairs)}\")\n",
        "print(f\"First 5 example of word_pairs is {word_pairs[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB75HBCt0Swd"
      },
      "source": [
        "## 6. Building the Vocabulary\n",
        "\n",
        "To work with word vectors, we need to create a vocabulary that maps each unique word to an index. We'll also filter out rare words that appear less than a certain number of times in the corpus.\n",
        "\n",
        "### 6.1 Collecting All Words\n",
        "\n",
        "First, let's collect all words in our corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htinJiMPkkRE",
        "outputId": "2d862c5f-d413-471e-fb0e-01e3f2ce8044"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77597"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# we have to make vocabulary\n",
        "# pretty print!\n",
        "\n",
        "sentence = corpus[0]\n",
        "entire_words = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    for word in sentence:\n",
        "        entire_words.append(word)\n",
        "\n",
        "len(entire_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N55b-TDm0Swd"
      },
      "source": [
        "\n",
        "### 6.2 Finding Unique Words\n",
        "\n",
        "Now, let's find the unique words in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBFCjeslgDe",
        "outputId": "05acaf6f-9678-41c1-9c2f-f5a1e1965c09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6038"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# we have to get the \"unique\" item among total words\n",
        "\n",
        "unique_words = set(entire_words)\n",
        "len(unique_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVAEC7_e0Swe"
      },
      "source": [
        "### 6.3 Converting to a List and Sorting\n",
        "\n",
        "We'll convert the set of unique words to a sorted list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fDJNrHdhl_dk",
        "outputId": "084a2bbe-e6d0-44db-c652-be7898118727"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# vocab_set[0] # set is not subscriptable because it has no order\n",
        "\n",
        "unique_words = sorted(list(unique_words))\n",
        "unique_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otFGgdvr0Swe"
      },
      "source": [
        "### 6.4 Filtering by Frequency\n",
        "\n",
        "Now, let's filter out rare words that occur less than a specified number of times:\n",
        "- We can use the `Counter` class from the `collections` module to count the frequency of each word in the corpus.\n",
        "- Caution on `alist.sort()` will return `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOkBSjrkmNE4",
        "outputId": "86a3573a-8f9a-4e80-f4eb-4c151d16e0db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'able',\n",
              " 'abou',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'added',\n",
              " 'afford',\n",
              " 'afraid',\n",
              " 'after']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# how can we filter the vocab by its frequency?\n",
        "filtered_vocab = None\n",
        "# you can use word counter as dictionary\n",
        "# In python dictionary, dict.keys() gives keys, and dict.values() give values,\n",
        "# dict.items() give (key, value)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "word_counter = Counter(entire_words)\n",
        "\n",
        "word_counter.most_common(10)\n",
        "word_counter['harry']\n",
        "\n",
        "threshold = 5\n",
        "filtered_vocab = []\n",
        "for k, v in word_counter.items():\n",
        "    if v > threshold:\n",
        "        filtered_vocab.append(k)\n",
        "\n",
        "filtered_vocab.sort()\n",
        "filtered_vocab[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyAzG6Kl0Swe"
      },
      "source": [
        "## 7. Filtering Word Pairs\n",
        "\n",
        "Now that we have our filtered vocabulary, we need to filter our word pairs to only include words that are in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUS6U7y7opUp",
        "outputId": "c4f28355-a1cb-413a-e054-4de5af4a6290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 282372/282372 [00:00<00:00, 542551.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter the word_pairs using the vocab\n",
        "# word_pairs, filtered_vocab\n",
        "# word_pairs is a list of [word_a, word_b]\n",
        "\n",
        "'''\n",
        "filtered_pairs = []\n",
        "\n",
        "for pair in tqdm(word_pairs):\n",
        "    a, b = pair\n",
        "    if (a in filtered_vocab) and (b in filtered_vocab):\n",
        "        filtered_pairs.append(pair)\n",
        "'''\n",
        "\n",
        "filtered_pairs = []\n",
        "vocab_set = set(filtered_vocab)\n",
        "\n",
        "for pair in tqdm(word_pairs):\n",
        "    a, b = pair\n",
        "    if (a in vocab_set) and (b in vocab_set):\n",
        "        filtered_pairs.append(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U-o4UucOrcem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1a8c0c-5b57-4b6c-a233-5a28302b89ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226846"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# implement same algorithm with list comprehension\n",
        "\n",
        "filtered_word_pairs = [pair for pair in word_pairs if ((pair[0] in vocab_set) and (pair[1] in vocab_set))]\n",
        "len(filtered_word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_8ch59ps_P",
        "outputId": "1ed7dc27-d8fc-4635-9f27-39388bb8f12c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226846, 282372)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(filtered_pairs), len(word_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8xqjaZ0Swf"
      },
      "source": [
        "## 8. Converting Words to Indices\n",
        "\n",
        "For efficiency, we'll convert our words to indices according to their position in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFJfhOznqyi-",
        "outputId": "479c3de6-41bc-4919-b4a7-280e8f8c52ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# convert word into index of vocab\n",
        "filtered_vocab.index('harry')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlG7v_Bq0Swf"
      },
      "source": [
        "This is inefficient because `list.index()` has to scan the list every time. Let's use a dictionary for faster lookups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D8n16VitIHP",
        "outputId": "3a875b4e-82d3-4191-89b1-3071bb5c091a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# we can make it faster\n",
        "# use dictionary to find the index of string\n",
        "\n",
        "# dictionary나 set이 탐색에 훨씬 유리함 (빠름)\n",
        "\n",
        "word2idx = dict()\n",
        "for idx, word in enumerate(filtered_vocab):\n",
        "    word2idx[word] = idx\n",
        "\n",
        "word2idx['harry']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NLZbbaI0Swf"
      },
      "source": [
        "Now, let's convert our word pairs to index pairs more efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utXemuOgt8-o"
      },
      "outputs": [],
      "source": [
        "index_pairs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgQ_oSNGuZAd"
      },
      "outputs": [],
      "source": [
        "# Why we don't need idx2tok?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md7eGNTE0Swf"
      },
      "source": [
        "## 9. Creating Initial Word Vectors\n",
        "\n",
        "Now we'll create random vectors for each word in our vocabulary. These vectors will be adjusted during training:\n",
        "- We can use `torch.randn` to create random vectors that follow normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygV93qzDu4Ls"
      },
      "outputs": [],
      "source": [
        "# we have to make random vectors for each word in the vocab\n",
        "# we also have to decide the dimension of the vector\n",
        "\n",
        "dim = None\n",
        "vocab_size = None\n",
        "\n",
        "word_vectors = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmZcT53rvwW2"
      },
      "outputs": [],
      "source": [
        "# what is the vector for harry?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXCRRAJV0Swg"
      },
      "source": [
        "## 10. Understanding Word Relationships with Dot Products\n",
        "\n",
        "The core of Word2Vec is using dot products to measure relationships between words. Let's explore this concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4bEKFvUxbVv"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(sci_mode=False) # Do this to avoid scientific notation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCMWmD6Suhy"
      },
      "source": [
        "## Dot Product\n",
        "- Assume we have two vectors $a$ and $b$.\n",
        "  - $a = [a_1, a_2, a_3, a_4, ..., a_n]$\n",
        "  - $b = [b_1, b_2, b_3, b_4, ..., b_n]$\n",
        "- $a \\cdot b$ = $\\sum _{i=1}^n a_ib_i$  = $a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + ... + a_nb_n$\n",
        "\n",
        "Let's calculate the dot product between \"harry\" and \"potter\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsse-jUrw6c2"
      },
      "outputs": [],
      "source": [
        "# calculate P(potter|harry)\n",
        "\n",
        "dot_product_value_between_potter_harry = None\n",
        "dot_product_value_between_potter_harry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZrLEY36yBNZ"
      },
      "outputs": [],
      "source": [
        "# we can get the dot product value for every other words in the vocab\n",
        "# to get  P(word | harry)\n",
        "word_dot_dict = {}\n",
        "\n",
        "word_dot_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyw8p0IX0Swl"
      },
      "source": [
        "Now, let's convert these dot products to probabilities using the softmax function:\n",
        "- We have to convert our prediction into probability distribution to get P(word|harry) so that sum of [P(a|harry), ..., P(potter|harry), ... P(ron|harry), ... ] = 1\n",
        "- current dot product value is any real number, sometimes called as logit\n",
        "  - logit from logistic regression. Some values that are not yet converted to 0-1 or value before sigmoid function\n",
        "  - every probability should be in range (0, 1) (greater than 0, smaller than 1)\n",
        "  - this can be handled by taking exponential of dot product values, divided by total sum\n",
        "  - This function is called **Softmax**\n",
        "\n",
        "- Why we use exponential?\n",
        "  - Because we want to make every probability in positive range while preserving the order\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ1PUvuLyv6r"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "\n",
        "word_prob_dict = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpXH7RxSbwUO"
      },
      "outputs": [],
      "source": [
        "# Get P(potter|harry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxz9SJhSuhy"
      },
      "source": [
        "## 13. Efficient Matrix Operations\n",
        "![img](https://mkang32.github.io/images/python/khan_academy_matrix_product.png)\n",
        "\n",
        "Instead of calculating dot products one by one, we can use matrix multiplication for efficiency:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCFajiDkb44W"
      },
      "outputs": [],
      "source": [
        "# get dot product result for every word in the vocabulary\n",
        "\n",
        "# first, make vector_of_harry into matrix format\n",
        "\n",
        "# do matrix multiplication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev5Ad-XP0Swm"
      },
      "source": [
        "Let's verify that our matrix multiplication gives the same result as individual dot products:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEDL5BNy0Swm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx2JeX960Swm"
      },
      "source": [
        "Now let's implement the complete softmax calculation using matrix operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5USdps5CfKzq"
      },
      "outputs": [],
      "source": [
        "# convert dot product result into exponential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNq0gqz0fntB"
      },
      "outputs": [],
      "source": [
        "# get the sum of exponential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruV4_myDgLvG"
      },
      "outputs": [],
      "source": [
        "# divide exponential value with sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1XZ6n1R0Swn"
      },
      "source": [
        "## 14. Creating a Probability Function\n",
        "\n",
        "Let's create a function to calculate probabilities efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF4gutuEgntj"
      },
      "outputs": [],
      "source": [
        "def get_probs(query_vectors, entire_vectors):\n",
        "  return None\n",
        "\n",
        "# get_probs(mat_of_harry, word_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifEQ07Y90Swn"
      },
      "source": [
        "## 15. Preparing for Training\n",
        "\n",
        "Before training our Word2Vec model, we need to split our dataset into training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gjCSY3DoaFg"
      },
      "outputs": [],
      "source": [
        "# Now we can train the word2vec\n",
        "\n",
        "# Let's think about training pairs\n",
        "index_pairs # this is our dataset. It's list of list of two integer\n",
        "# two integer means a pair of neighboring words\n",
        "\n",
        "# Training set and Test set\n",
        "# To validate that our model can solve 'unseen' problems\n",
        "# So we have to split the dataset before training.\n",
        "\n",
        "# To randomly split the dataset, we will first shuffle the dataset\n",
        "\n",
        "# random.shuffle(index_pairs) # this will shuffle the list items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC0QF3NZp9Rh"
      },
      "outputs": [],
      "source": [
        "len(train_set), len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twBl3V910Swo"
      },
      "source": [
        "## 16. Training the Word2Vec Model\n",
        "\n",
        "Now we'll train our Word2Vec model using batched gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpUM3aiiqG8e"
      },
      "outputs": [],
      "source": [
        "# making batch from train_set\n",
        "# Batch is a set of training samples, that are calculated together\n",
        "# And also we update the model after one single batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjN7KOEj0Swo"
      },
      "source": [
        "## 17. Evaluating the Training\n",
        "\n",
        "Let's visualize the training loss to see if our model is learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJHSV8zbyLYu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nx6YIv50Swo"
      },
      "source": [
        "## 18. Testing the Model\n",
        "\n",
        "Now we'll test our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDP9aR48zdJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7pNTOvA0Swp"
      },
      "source": [
        "## 19. Exploring Learned Word Relationships\n",
        "\n",
        "Let's explore what our model has learned by finding the words most closely related to \"harry\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YaCDNq_0hFc"
      },
      "outputs": [],
      "source": [
        "# P(potter|harry)?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}